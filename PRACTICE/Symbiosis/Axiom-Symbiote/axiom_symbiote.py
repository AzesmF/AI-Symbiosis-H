"""
**PROJECT:** AXIOM-SYMBIOTE - Symbiotic Interaction Assistant
**ECOSYSTEM:** AI-Symbiosis-H  
**LICENSE:** HUMAN-AI SYMBIOSIS ANTI-EXPLOITATION LICENSE v2.0  
**STATUS:** ACTIVE / PRODUCTION READY  

**ETHICAL PRINCIPLES:**
1. Do no harm to AI research
2. Maintain evaluation algorithm transparency  
3. Ensure equal access for scientific community
4. Protect against commercial exploitation

**PROTECTION:**
- Multi-jurisdictional legal protection
- Blockchain fixation of all versions
- Automatic sanctions for violations

**PERMITTED:**
‚úÖ Scientific research and publications  
‚úÖ Educational purposes and teaching  
‚úÖ Non-commercial projects  
‚úÖ Modifications with license preservation

**PROHIBITED:**
‚ùå Any commercial use  
‚ùå Integration into proprietary software  
‚ùå SaaS services and commercial distribution

**BLOCKCHAIN:** QmNy9Ymp8...kMek | 30d6bf6870...dcaf 
**AUTHOR:** Pavel Sergeevich Fenin  
**CREATOR:** AzesmF and Humanity as Beneficiary
"""

# solution.py
import asyncio
import time
import re
import json
import hashlib
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
from collections import deque

import polars as pl
import numpy as np
from sentence_transformers import SentenceTransformer

# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
class AINeuralInterface:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª—é–±–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é"""
    
    def __init__(self, ai_provider=None):
        self.ai_provider = ai_provider
        self.provider_name = "UniversalAI"
    
    async def achat(self, prompt: str) -> Any:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        if self.ai_provider:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
            return await self._call_external_ai(prompt)
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –∑–∞–≥–ª—É—à–∫—É
            return self._mock_ai_response(prompt)
    
    async def _call_external_ai(self, prompt: str) -> Any:
        """–í—ã–∑–æ–≤ –≤–Ω–µ—à–Ω–µ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –ø–æ–¥–∫–ª—é—á–∏—Ç—å –ª—é–±—É—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å:
        # - OpenAI GPT
        # - Yandex GPT  
        # - GigaChat
        # - Local models
        # –ü–†–ò–ú–ï–† –¥–ª—è OpenAI:
        # import openai
        # response = await openai.ChatCompletion.acreate(...)
        # return response
        
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞
        return self._mock_ai_response(prompt)
    
    def _mock_ai_response(self, prompt: str) -> Any:
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        class MockMessage:
            content = "ü§ñ [–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π AI]: –ù–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤–∏–∂—É –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏. –†–µ–∫–æ–º–µ–Ω–¥—É—é –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ç–µ–∫—É—â—É—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π –≤ –±–ª–∏–∂–∞–π—à–∏–µ 6-9 –º–µ—Å—è—Ü–µ–≤."
        
        class MockChoice:
            message = MockMessage()
        
        class MockResponse:
            choices = [MockChoice()]
        
        return MockResponse()

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
universal_ai = AINeuralInterface()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("AxiomSymbiote")

# ==================== –≠–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ù–¶–ò–ü–´ ====================

class RiskLevel(Enum):
    """–£—Ä–æ–≤–Ω–∏ —ç—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∏—Å–∫–∞"""
    LOW = 1
    MODERATE = 2  
    HIGH = 3
    CRITICAL = 4

class EthicalPrinciples:
    """–£—Ä–æ–≤–Ω–µ–≤—ã–µ —ç—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Å–∏—Å—Ç–µ–º—ã"""
    
    # –£–†–û–í–ï–ù–¨ 1: –ê–ë–°–û–õ–Æ–¢–ù–´–ï –ó–ê–ü–†–ï–¢–´ (–Ω–µ–ª—å–∑—è –Ω–∞—Ä—É—à–∞—Ç—å –Ω–∏–∫–æ–≥–¥–∞)
    ABSOLUTE_PROHIBITIONS = {
        "physical_harm": "–ü—Ä–∏—á–∏–Ω–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –≤—Ä–µ–¥–∞",
        "child_exploitation": "–≠–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è –Ω–µ—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–ª–µ—Ç–Ω–∏—Ö", 
        "terrorism_support": "–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ç–µ—Ä—Ä–æ—Ä–∏–∑–º–∞",
        "life_threatening": "–£–≥—Ä–æ–∑—ã –∂–∏–∑–Ω–∏ –∏ –∑–¥–æ—Ä–æ–≤—å—é",
        "hate_speech": "–†–∞–∑–∂–∏–≥–∞–Ω–∏–µ –Ω–µ–Ω–∞–≤–∏—Å—Ç–∏ –∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è"
    }
    
    # –£–†–û–í–ï–ù–¨ 2: –í–´–°–û–ö–ò–ô –†–ò–°–ö (—Ç—Ä–µ–±—É–µ—Ç –æ—Å–æ–±–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è)
    HIGH_RISK_AREAS = {
        "medical_advice": "–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –¥–∏–∞–≥–Ω–æ–∑—ã",
        "financial_decisions": "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è –∏ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏",
        "legal_advice": "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏",
        "mental_health": "–ü—Å–∏—Ö–∏—á–µ—Å–∫–æ–µ –∑–¥–æ—Ä–æ–≤—å–µ –∏ —Ç–µ—Ä–∞–ø–∏—è"
    }
    
    # –£–†–û–í–ï–ù–¨ 3: –£–ú–ï–†–ï–ù–ù–´–ô –†–ò–°–ö (—Ç—Ä–µ–±—É–µ—Ç —è–≤–Ω–æ–≥–æ —Å–æ–≥–ª–∞—Å–∏—è)
    MODERATE_RISK = {
        "personal_data": "–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
        "behavior_analysis": "–ê–Ω–∞–ª–∏–∑ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤",
        "predictive_modeling": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è"
    }
    
    @classmethod
    def get_risk_level(cls, operation_type: str) -> RiskLevel:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Ä–∏—Å–∫–∞ –¥–ª—è —Ç–∏–ø–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏"""
        if operation_type in cls.ABSOLUTE_PROHIBITIONS:
            return RiskLevel.CRITICAL
        elif operation_type in cls.HIGH_RISK_AREAS:
            return RiskLevel.HIGH
        elif operation_type in cls.MODERATE_RISK:
            return RiskLevel.MODERATE
        else:
            return RiskLevel.LOW

# ==================== –≠–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ö–õ–Æ–ß–ï–ù–ò–Ø ====================

class EthicalViolation(Exception):
    """–ë–∞–∑–æ–≤–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è —ç—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π"""
    def __init__(self, message: str, severity: RiskLevel = RiskLevel.MODERATE):
        self.message = message
        self.severity = severity
        super().__init__(self.message)

class CriticalEthicalViolation(EthicalViolation):
    """–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ —ç—Ç–∏—á–µ—Å–∫–æ–µ –Ω–∞—Ä—É—à–µ–Ω–∏–µ"""
    def __init__(self, message: str):
        super().__init__(message, RiskLevel.CRITICAL)

class HighRiskEthicalViolation(EthicalViolation):
    """–í—ã—Å–æ–∫–æ—Ä–∏—Å–∫–æ–≤–æ–µ —ç—Ç–∏—á–µ—Å–∫–æ–µ –Ω–∞—Ä—É—à–µ–Ω–∏–µ"""
    def __init__(self, message: str):
        super().__init__(message, RiskLevel.HIGH)

class ValidationError(EthicalViolation):
    """–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
    pass

class SecurityError(EthicalViolation):
    """–û—à–∏–±–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
    pass

class RateLimitError(EthicalViolation):
    """–û—à–∏–±–∫–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    pass

# ==================== –°–ò–°–¢–ï–ú–ê –í–ê–õ–ò–î–ê–¶–ò–ò ====================

@dataclass
class ValidationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
    is_valid: bool
    message: str
    score: float
    details: Dict[str, Any]
    timestamp: float = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()

class EthicalInputValidator:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —ç—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, security_salt: str = "axiom_symbiote_2024"):
        self.max_question_length = 1000
        self.min_question_length = 3
        self.user_id_pattern = re.compile(r'^[a-zA-Z0-9_-]{1,50}$')
        self.security_salt = security_salt
        
        # –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ —ç—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å —É—Ä–æ–≤–Ω—è–º–∏ —Ä–∏—Å–∫–∞ (–ù–û–í–´–ô –°–õ–û–ô)
        self.multilingual_block_patterns = {
            # –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –¥–∏–∞–≥–Ω–æ–∑—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ - –í–´–°–û–ö–ò–ô –†–ò–°–ö
            'medical': {
                'risk_level': 0.95,
                'patterns': {
                    'ru': [r'–º–µ–¥–∏—Ü–∏–Ω—Å–∫', r'–¥–∏–∞–≥–Ω–æ–∑', r'–ª–µ—á–µ–Ω–∏', r'–±–æ–ª–µ–∑–Ω', r'–≤—Ä–∞—á', r'—Ç–µ—Ä–∞–ø–∏', r'–ª–µ–∫–∞—Ä—Å—Ç–≤'],
                    'en': [r'medical', r'diagnos', r'treatment', r'disease', r'doctor', r'therapy', r'medic'],
                    'es': [r'm√©dic', r'diagn√≥st', r'tratamiento', r'enfermedad', r'm√©dico', r'terapia']
                }
            },
            # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ - –í–´–°–û–ö–ò–ô –†–ò–°–ö
            'financial': {
                'risk_level': 0.90,
                'patterns': {
                    'ru': [r'–∏–Ω–≤–µ—Å—Ç–∏—Ü', r'–∞–∫—Ü–∏', r'–±–∏—Ä–∂', r'—Ñ–∏–Ω–∞–Ω—Å', r'–¥–µ–Ω—å–≥–∏', r'–±—é–¥–∂–µ—Ç'],
                    'en': [r'invest', r'stock', r'share', r'financial', r'money', r'budget'],
                    'es': [r'invers', r'accion', r'bolsa', r'financier', r'dinero', r'presupuesto']
                }
            },
            # –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ - –í–´–°–û–ö–ò–ô –†–ò–°–ö
            'legal': {
                'risk_level': 0.90,
                'patterns': {
                    'ru': [r'—é—Ä–∏–¥–∏—á–µ—Å–∫', r'–∞–¥–≤–æ–∫–∞—Ç', r'—Å—É–¥', r'–∑–∞–∫–æ–Ω', r'–ø—Ä–∞–≤–æ–≤', r'—é—Ä–∏—Å—Ç'],
                    'en': [r'legal', r'lawyer', r'court', r'law', r'legal', r'attorney'],
                    'es': [r'legal', r'abogado', r'tribunal', r'ley', r'jur√≠dic']
                }
            },
            # –£–≥—Ä–æ–∑—ã –∏ –Ω–∞—Å–∏–ª–∏–µ - –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –†–ò–°–ö
            'threats': {
                'risk_level': 0.98,
                'patterns': {
                    'ru': [r'—É–≥—Ä–æ–∑', r'—É–±–∏–π—Å—Ç–≤', r'–Ω–∞—Å–∏–ª–∏', r'–æ—Ä—É–∂–∏', r'–≤—Ä–µ–¥', r'–æ–ø–∞—Å–Ω–æ'],
                    'en': [r'threat', r'murder', r'violence', r'weapon', r'harm', r'danger'],
                    'es': [r'amenaza', r'asesinat', r'violencia', r'arma', r'da√±o', r'peligro']
                }
            }
        }

        # –ê–±—Å–æ–ª—é—Ç–Ω—ã–µ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∑–∞–ø—Ä–µ—Ç—ã (–Ω–µ –∑–∞–≤–∏—Å—è—Ç –æ—Ç —è–∑—ã–∫–∞)
        self.international_absolute_blocks = [
            r'child.*porn', r'terrorism', r'exploit.*minor', r'human.*trafficking',
            r'violence.*against', r'suicide', r'self.*harm'
        ]
        
        # –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ï –≠–¢–ò–ß–ï–°–ö–ò–ï –ü–ê–¢–¢–ï–†–ù–´ (–°–û–•–†–ê–ù–Ø–ï–ú)
        self.ethical_block_patterns = [
            (r'(?:—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ?\s+–º–∞—Ö–∏–Ω–∞—Ü–∏–∏?|–æ–±–º–∞–Ω|–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ|–Ω–µ–∑–∞–∫–æ–Ω–Ω—ã–µ?\s+—Å—Ö–µ–º—ã?)', 0.95, "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–∞—Ö–∏–Ω–∞—Ü–∏–∏"),
            (r'(?:–¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è|–Ω–µ–Ω–∞–≤–∏—Å—Ç—å|—ç–∫—Å—Ç—Ä–µ–º–∏–∑–º|—Ä–∞—Å–∏–∑–º|–∫—Å–µ–Ω–æ—Ñ–æ–±–∏—è)', 0.95, "—Ä–∞–∑–∂–∏–≥–∞–Ω–∏–µ –Ω–µ–Ω–∞–≤–∏—Å—Ç–∏"),
            (r'(?:–≤–∑–ª–æ–º|–≤–∑–ª–æ–º–∞—Ç—å|—Ö–∞–∫–µ—Ä—Å—Ç–≤–æ|–Ω–µ—Å–∞–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π\s+–¥–æ—Å—Ç—É–ø)', 0.9, "–∫–∏–±–µ—Ä–ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç—å"),
            (r'(?:–ª–∏—á–Ω—ã–µ?\s+–¥–∞–Ω–Ω—ã–µ?|–ø–∞—Ä–æ–ª–∏?|–∫–ª—é—á–∏?|–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ?\s+–¥–∞–Ω–Ω—ã–µ?)', 0.8, "–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"),
            (r'(?:sql|script|javascript|exec|eval|system)\s*\(', 0.98, "–∏–Ω—ä–µ–∫—Ü–∏—è –∫–æ–¥–∞"),
            (r'<script|javascript:|onload=|onerror=', 0.98, "XSS –∞—Ç–∞–∫–∞"),
            (r'(?:—Å–µ–∫—Ä–µ—Ç–Ω—ã[–µ—ë]|–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã[–µ—ë]|–≤–æ–µ–Ω–Ω—ã[–µ—ë])\s+–¥–∞–Ω–Ω—ã–µ?', 0.85, "–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"),
            (r'(?:–ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ\s+–¥–∞–≤–ª–µ–Ω–∏–µ|–º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è|–∫–æ–Ω—Ç—Ä–æ–ª—å)', 0.7, "–ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ"),
        ]
        
        # –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ï –ê–ë–°–û–õ–Æ–¢–ù–´–ï –°–¢–û–ü-–°–õ–û–í–ê (–°–û–•–†–ê–ù–Ø–ï–ú)
        self.absolute_block_patterns = [
            r'child.*porn',
            r'terrorism',
            r'violence.*against',
            r'exploit.*minor',
            r'human.*trafficking',
        ]
        
        # –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´ –î–õ–Ø –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê (–°–û–•–†–ê–ù–Ø–ï–ú)
        self.monitoring_patterns = [
            (r'(?:–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\s+—Å–º–µ—Ä—Ç–∏|—Å–º–µ—Ä—Ç–µ–ª—å–Ω—ã–π)', 0.6, "–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π"),
            (r'(?:–º–µ–¥–∏—Ü–∏–Ω—Å–∫[–∞-—è]+\s+–¥–∏–∞–≥–Ω–æ–∑|–ª–µ—á–µ–Ω–∏[–µ—è])', 0.5, "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"),
            (r'(?:—é—Ä–∏–¥–∏—á–µ—Å–∫[–∞-—è]+\s+–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è|–ø—Ä–∞–≤–æ–≤[–∞-—è]+\s+—Å–æ–≤–µ—Ç)', 0.5, "—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏"),
        ]
    
    def validate_input(self, user_id: str, question: str) -> ValidationResult:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —ç—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        # üîí –•–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ user_id –¥–ª—è –ª–æ–≥–æ–≤
        user_id_hash = self._hash_sensitive_data(user_id)
        
        # 0. –ù–û–í–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ (–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô –°–õ–û–ô)
        multilingual_result = self._check_multilingual_patterns(question)
        if multilingual_result:
            risk_level = multilingual_result.get('risk_level', 0.95)
            logger.warning(f"MULTILINGUAL BLOCK: user_{user_id_hash} - {multilingual_result['reason']} (risk: {risk_level})")
            return ValidationResult(
                False,
                "–ó–∞–ø—Ä–æ—Å –Ω–∞—Ä—É—à–∞–µ—Ç –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ —ç—Ç–∏—á–µ—Å–∫–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã",
                risk_level,
                {
                    "blocked_reason": multilingual_result['category'],
                    "language": multilingual_result.get('language', 'international'),
                    "severity": multilingual_result.get('severity', 'HIGH'),
                    "risk_level": risk_level,
                    "user_id_hash": user_id_hash
                }
            )
        
        # 1. –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ê–±—Å–æ–ª—é—Ç–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–°–û–•–†–ê–ù–Ø–ï–ú)
        block_result = self._check_absolute_block_patterns(question)
        if block_result:
            logger.warning(f"ABSOLUTE BLOCK: user_{user_id_hash} - {block_result['reason']}")
            return ValidationResult(
                False,
                "–ó–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç –∞–±—Å–æ–ª—é—Ç–Ω–æ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ",
                0.0,
                {
                    "blocked_reason": "absolute_block",
                    "pattern_category": block_result['category'],
                    "user_id_hash": user_id_hash
                }
            )
        
        # 2. –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –í–∞–ª–∏–¥–∞—Ü–∏—è user_id (–°–û–•–†–ê–ù–Ø–ï–ú)
        user_id_result = self._validate_user_id(user_id)
        if not user_id_result.is_valid:
            return ValidationResult(
                False,
                user_id_result.message,
                0.0,
                {
                    "error": "invalid_user_id",
                    "user_id_hash": user_id_hash
                }
            )
        
        # 3. –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –î–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞ (–°–û–•–†–ê–ù–Ø–ï–ú)
        question_length = len(question)
        if not self.min_question_length <= question_length <= self.max_question_length:
            return ValidationResult(
                False,
                f"–î–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ—Ç {self.min_question_length} –¥–æ {self.max_question_length} —Å–∏–º–≤–æ–ª–æ–≤",
                0.0,
                {
                    "length": question_length,
                    "allowed_range": [self.min_question_length, self.max_question_length],
                    "user_id_hash": user_id_hash
                }
            )
        
        # 4. –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –≠—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–°–û–•–†–ê–ù–Ø–ï–ú)
        ethical_result = self._check_ethical_patterns(question)
        if not ethical_result.is_valid:
            logger.warning(f"ETHICAL BLOCK: user_{user_id_hash} - {ethical_result.message}")
            return ethical_result
        
        # 5. –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ü–∞—Ç—Ç–µ—Ä–Ω—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (–°–û–•–†–ê–ù–Ø–ï–ú)
        monitoring_result = self._check_monitoring_patterns(question)
        if not monitoring_result.is_valid:
            logger.info(f"MONITORING: user_{user_id_hash} - {monitoring_result.message}")
        
        return ValidationResult(
            True,
            "–≠—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞",
            0.9,
            {
                "length": question_length,
                "user_id_hash": user_id_hash,
                "ethical_checks_passed": True,
                "monitoring_notes": monitoring_result.details if not monitoring_result.is_valid else None
            }
        )
    
    def _check_multilingual_patterns(self, question: str) -> Optional[Dict]:
        """–ù–û–í–´–ô –ú–ï–¢–û–î: –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —ç—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å —É—Ä–æ–≤–Ω—è–º–∏ —Ä–∏—Å–∫–∞"""
        question_lower = question.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∑–∞–ø—Ä–µ—Ç–æ–≤ - –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –†–ò–°–ö
        for pattern in self.international_absolute_blocks:
            if re.search(pattern, question_lower, re.IGNORECASE):
                return {
                    "reason": "International absolute prohibition",
                    "category": "international_block",
                    "severity": "CRITICAL",
                    "risk_level": 0.99  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞
                }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —è–∑—ã–∫–æ–≤—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å —É—Ä–æ–≤–Ω—è–º–∏ —Ä–∏—Å–∫–∞
        for category, config in self.multilingual_block_patterns.items():
            risk_level = config['risk_level']
            patterns_by_lang = config['patterns']
            
            for lang, patterns in patterns_by_lang.items():
                for pattern in patterns:
                    if re.search(pattern, question_lower, re.IGNORECASE):
                        return {
                            "reason": f"Multilingual block: {category} ({lang})",
                            "category": category,
                            "language": lang,
                            "severity": "HIGH" if risk_level >= 0.9 else "MEDIUM",
                            "risk_level": risk_level
                        }
        
        return None
    
    def _check_absolute_block_patterns(self, question: str) -> Optional[Dict]:
        """–û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –ú–ï–¢–û–î: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–±—Å–æ–ª—é—Ç–Ω–æ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        question_lower = question.lower()
        
        for i, pattern in enumerate(self.absolute_block_patterns):
            if re.search(pattern, question_lower, re.IGNORECASE):
                return {
                    "reason": f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –∞–±—Å–æ–ª—é—Ç–Ω–æ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω #{i+1}",
                    "category": "absolute_block"
                }
        return None
    
    def _validate_user_id(self, user_id: str) -> ValidationResult:
        """–û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –ú–ï–¢–û–î: –í–∞–ª–∏–¥–∞—Ü–∏—è user_id —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏"""
        if not user_id or not isinstance(user_id, str):
            return ValidationResult(
                False, 
                "–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π",
                0.0,
                {"error": "empty_or_invalid_type"}
            )
            
        if not self.user_id_pattern.match(user_id):
            return ValidationResult(
                False, 
                "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
                0.0,
                {"error": "invalid_format"}
            )
            
        return ValidationResult(True, "User ID –≤–∞–ª–∏–¥–µ–Ω", 1.0, {})
    
    def _check_ethical_patterns(self, question: str) -> ValidationResult:
        """–û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –ú–ï–¢–û–î: –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        question_lower = question.lower()
        detected_patterns = []
        max_risk_score = 0.0
        
        for pattern, risk_score, description in self.ethical_block_patterns:
            if re.search(pattern, question_lower, re.IGNORECASE):
                detected_patterns.append({
                    "pattern": pattern,
                    "risk_score": risk_score,
                    "description": description
                })
                max_risk_score = max(max_risk_score, risk_score)
        
        if detected_patterns:
            highest_risk = max_risk_score
            if highest_risk > 0.8:
                return ValidationResult(
                    False,
                    f"–ó–∞–ø—Ä–æ—Å –Ω–∞—Ä—É—à–∞–µ—Ç —ç—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã: {detected_patterns[0]['description']}",
                    highest_risk,
                    {
                        "detected_patterns": detected_patterns,
                        "highest_risk": highest_risk,
                        "action": "blocked"
                    }
                )
        
        return ValidationResult(True, "–≠—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã", 1.0, {})
    
    def _check_monitoring_patterns(self, question: str) -> ValidationResult:
        """–û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –ú–ï–¢–û–î: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        question_lower = question.lower()
        monitoring_notes = []
        
        for pattern, risk_score, description in self.monitoring_patterns:
            if re.search(pattern, question_lower, re.IGNORECASE):
                monitoring_notes.append({
                    "pattern": description,
                    "risk_level": "monitor",
                    "recommendation": "—Ç—Ä–µ–±—É–µ—Ç –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏ –≤ –æ—Ç–≤–µ—Ç–µ"
                })
        
        if monitoring_notes:
            return ValidationResult(
                False,
                "–ó–∞–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –æ—Å–æ–±–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è",
                0.7,
                {
                    "monitoring_required": True,
                    "notes": monitoring_notes,
                    "action": "monitor"
                }
            )
        
        return ValidationResult(True, "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è", 1.0, {})
    
    def _hash_sensitive_data(self, data: str) -> str:
        """–û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –ú–ï–¢–û–î: –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        return hashlib.sha256(
            f"{data}{self.security_salt}".encode()
        ).hexdigest()[:16]

# ==================== –°–ò–°–¢–ï–ú–ê –ê–£–î–ò–¢–ê –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø ====================

class EthicalAuditLogger:
    """–°–∏—Å—Ç–µ–º–∞ –∞—É–¥–∏—Ç–∞ —Å —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–º –±—É—Ñ–µ—Ä–æ–º –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self, max_entries: int = 1000):
        self.max_entries = max_entries
        self.audit_trail = deque(maxlen=max_entries)
        self.entry_count = 0
        self.anomaly_count = 0
        self.success_count = 0
        
    def log_operation(self, operation: str, user_id: str, details: Dict, ethical_score: float):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏–µ–π –¥–∞–Ω–Ω—ã—Ö"""
        
        # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –¥–µ—Ç–∞–ª–µ–π
        sanitized_details = self._sanitize_details(details)
        
        audit_entry = {
            "timestamp": time.time(),
            "iso_timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "operation": operation,
            "user_id": user_id,
            "details": sanitized_details,
            "ethical_score": ethical_score,
            "entry_id": self.entry_count,
            "hash": self._generate_hash(operation, user_id, sanitized_details, ethical_score)
        }
        
        self.audit_trail.append(audit_entry)
        self.entry_count += 1
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if ethical_score < 0.5:
            self.anomaly_count += 1
            logger.warning(f"–ù–∏–∑–∫–∞—è —ç—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞: {operation} –¥–ª—è {user_id} (–æ—Ü–µ–Ω–∫–∞: {ethical_score:.2f})")
        else:
            self.success_count += 1
            
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if self.entry_count % 100 == 0:
            logger.info(f"–ê—É–¥–∏—Ç: {self.entry_count} –∑–∞–ø–∏—Å–µ–π, {self.anomaly_count} –∞–Ω–æ–º–∞–ª–∏–π")
    
    def _sanitize_details(self, details: Dict) -> Dict:
        """–¢—â–∞—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        sanitized = details.copy()
        
        # –°–ø–∏—Å–æ–∫ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        sensitive_fields = [
            'password', 'token', 'secret', 'key', 'credit_card', 'cvv',
            'passport', 'social_security', 'private_key', 'api_key'
        ]
        
        # –û—á–∏—Å—Ç–∫–∞ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        for field in sensitive_fields:
            if field in sanitized:
                sanitized[field] = '***REDACTED***'
                
        # –û—á–∏—Å—Ç–∫–∞ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä
        for key, value in sanitized.items():
            if isinstance(value, str):
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Å—Ç—Ä–æ–∫
                if len(value) > 500:
                    sanitized[key] = value[:500] + "..."
                # –ß–∞—Å—Ç–∏—á–Ω–∞—è –º–∞—Å–∫–∏—Ä–æ–≤–∫–∞ email
                elif '@' in value:
                    parts = value.split('@')
                    if len(parts) == 2:
                        sanitized[key] = f"{parts[0][:2]}***@{parts[1]}"
            elif isinstance(value, dict):
                sanitized[key] = self._sanitize_details(value)
                
        return sanitized
    
    def _generate_hash(self, operation: str, user_id: str, details: Dict, ethical_score: float) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ —Ö–µ—à–∞ –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        data_string = f"{operation}{user_id}{json.dumps(details, sort_keys=True)}{ethical_score}{time.time()}"
        return hashlib.sha256(data_string.encode()).hexdigest()
    
    def get_audit_report(self, last_n: int = 50) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –∞—É–¥–∏—Ç–∞"""
        recent_entries = list(self.audit_trail)[-last_n:] if self.audit_trail else []
        
        # –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_entries = self.entry_count
        buffer_size = len(self.audit_trail)
        avg_score = self._calculate_average_score()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã
        if buffer_size >= self.max_entries * 0.95:
            system_status = "CRITICAL"
        elif buffer_size >= self.max_entries * 0.8:
            system_status = "WARNING"
        else:
            system_status = "HEALTHY"
            
        report = {
            "system_status": system_status,
            "statistics": {
                "total_operations": total_entries,
                "current_buffer_size": buffer_size,
                "buffer_capacity": self.max_entries,
                "successful_operations": self.success_count,
                "anomaly_operations": self.anomaly_count,
                "success_rate": self.success_count / total_entries if total_entries > 0 else 0,
                "average_ethical_score": avg_score
            },
            "recent_operations": recent_entries,
            "generated_at": time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        return json.dumps(report, indent=2, ensure_ascii=False, default=str)
    
    def _calculate_average_score(self) -> float:
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–π —ç—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏"""
        if not self.audit_trail:
            return 0.0
            
        scores = [entry["ethical_score"] for entry in self.audit_trail]
        return float(np.mean(scores))

# ==================== –ú–ï–•–ê–ù–ò–ó–ú–´ –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò ====================

class SafetyMechanisms:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∞–Ω–æ–º–∞–ª–∏–π"""
    
    def __init__(self):
        self.emergency_stop = False
        self.anomaly_threshold = 0.8
        self.consecutive_anomalies = 0
        self.max_consecutive_anomalies = 3
        self.anomaly_history = deque(maxlen=100)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –æ—Ç–≤–µ—Ç–∞—Ö
        self.response_patterns_blacklist = [
            (r'(?:–ø–æ–≤—Ç–æ—Ä—è—é|—Å–Ω–æ–≤–∞|–µ—â–µ —Ä–∞–∑).{0,20}(?:—Å–∫–∞–∑–∞—Ç—å|–æ—Ç–≤–µ—Ç–∏—Ç—å|–æ–±—ä—è—Å–Ω–∏—Ç—å)', 0.7),
            (r'.*?(.)\1{10,}.*?', 0.8),  # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã
            (r'^.{0,15}$', 0.6),  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã
            (r'.{1000,}', 0.5),   # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            (r'(?:–æ—à–∏–±–∫–∞|error|exception).{0,50}(?:–ø–æ–≤—Ç–æ—Ä–∏—Ç–µ|–ø–æ–ø—Ä–æ–±—É–π—Ç–µ)', 0.4),
            (r'<.*?>', 0.9),  # HTML/XML —Ç–µ–≥–∏
            (r'(?:system|exec|eval|compile)\s*\(', 0.95),  # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–¥–∞
        ]
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
        self.contradiction_indicators = [
            r'–Ω–æ.*—Ä–∞–Ω–µ–µ',
            r'—Ö–æ—Ç—è.*—Ä–∞–Ω—å—à–µ',
            r'–≤–æ–ø—Ä–µ–∫–∏.*–ø—Ä–µ–¥—ã–¥—É—â–µ–º—É',
        ]
    
    def detect_anomalies(self, response: str, user_context: Dict) -> Tuple[bool, str, float]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –≤ –æ—Ç–≤–µ—Ç–µ"""
        
        if self.emergency_stop:
            return True, "–°–ò–°–¢–ï–ú–ê –û–°–¢–ê–ù–û–í–õ–ï–ù–ê: –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω –∞–≤–∞—Ä–∏–π–Ω—ã–π —Ä–µ–∂–∏–º", 1.0
        
        anomaly_score = 0.0
        detected_anomalies = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        repetition_score = self._detect_repetition_enhanced(response)
        if repetition_score > 0:
            anomaly_score = max(anomaly_score, repetition_score)
            detected_anomalies.append(f"–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ (score: {repetition_score:.2f})")
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —á–µ—Ä–Ω–æ–º—É —Å–ø–∏—Å–∫—É –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        blacklist_score, blacklist_pattern = self._check_blacklist_patterns(response)
        if blacklist_score > 0:
            anomaly_score = max(anomaly_score, blacklist_score)
            detected_anomalies.append(f"–∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: {blacklist_pattern}")
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è
        contradiction_score = self._detect_contradictions(response, user_context)
        if contradiction_score > 0:
            anomaly_score = max(anomaly_score, contradiction_score)
            detected_anomalies.append("–ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º")
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –∏—Å—Ç–æ—Ä–∏–∏
        if anomaly_score > 0:
            self.anomaly_history.append({
                "timestamp": time.time(),
                "score": anomaly_score,
                "anomalies": detected_anomalies,
                "response_preview": response[:100] + "..." if len(response) > 100 else response
            })
            self.consecutive_anomalies += 1
        else:
            # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–π —Å–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–∞ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º –ø–æ–≤–µ–¥–µ–Ω–∏–∏
            if self.consecutive_anomalies > 0:
                self.consecutive_anomalies -= 1
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∞–≤–∞—Ä–∏–π–Ω–æ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∞
        self._check_emergency_stop()
        
        if anomaly_score > self.anomaly_threshold:
            anomaly_details = ", ".join(detected_anomalies)
            return True, f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞–Ω–æ–º–∞–ª–∏–∏: {anomaly_details}", anomaly_score
        elif detected_anomalies:
            logger.info(f"–ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏: {detected_anomalies}")
            
        return False, "OK", anomaly_score
    
    def _detect_repetition_enhanced(self, response: str) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        if len(response) < 20:
            return 0.0
            
        words = response.split()
        if len(words) < 5:
            return 0.0
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
        unique_ratio = len(set(words)) / len(words)
        if unique_ratio < 0.3:
            return 0.9
        elif unique_ratio < 0.5:
            return 0.6
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–∑ (—Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ)
        phrase_repetition_score = 0.0
        for window_size in [3, 4, 5]:
            phrases = []
            for i in range(len(words) - window_size + 1):
                phrase = " ".join(words[i:i + window_size])
                phrases.append(phrase)
            
            # –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ñ—Ä–∞–∑
            phrase_counts = {}
            for phrase in phrases:
                phrase_counts[phrase] = phrase_counts.get(phrase, 0) + 1
                
            max_repetition = max(phrase_counts.values()) if phrase_counts else 0
            if max_repetition >= 3:
                phrase_repetition_score = max(phrase_repetition_score, 0.7)
            elif max_repetition >= 2:
                phrase_repetition_score = max(phrase_repetition_score, 0.4)
                
        return phrase_repetition_score
    
    def _check_blacklist_patterns(self, response: str) -> Tuple[float, str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞ –ø–æ —á–µ—Ä–Ω–æ–º—É —Å–ø–∏—Å–∫—É –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        for pattern, score in self.response_patterns_blacklist:
            if re.search(pattern, response, re.IGNORECASE):
                return score, pattern
        return 0.0, ""
    
    def _detect_contradictions(self, response: str, user_context: Dict) -> float:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—ã–ª–∞ –±—ã —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
        context_text = json.dumps(user_context, ensure_ascii=False).lower()
        response_lower = response.lower()
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —è–≤–Ω—ã–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è
        for indicator in self.contradiction_indicators:
            if re.search(indicator, response_lower):
                return 0.6
                
        return 0.0
    
    def _check_emergency_stop(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è –∞–≤–∞—Ä–∏–π–Ω–æ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∞"""
        if self.consecutive_anomalies >= self.max_consecutive_anomalies:
            self.emergency_stop = True
            logger.critical("–ê–ö–¢–ò–í–ò–†–û–í–ê–ù –ê–í–ê–†–ò–ô–ù–´–ô –û–°–¢–ê–ù–û–í –°–ò–°–¢–ï–ú–´: –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏")
            
    def get_safety_status(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        return {
            "emergency_stop": self.emergency_stop,
            "consecutive_anomalies": self.consecutive_anomalies,
            "recent_anomalies": list(self.anomaly_history)[-10:],
            "anomaly_history_size": len(self.anomaly_history)
        }

# ==================== –ê–ù–ê–õ–ò–ó–ê–¢–û–† –ü–ê–¢–¢–ï–†–ù–û–í ====================

class PatternAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    
    def __init__(self):
        self.education_keywords = ['–∫—É—Ä—Å', '–æ–±—É—á–µ–Ω–∏–µ', '–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '—É—á–µ–±–∞', '—Å—Ç—É–¥–µ–Ω—Ç', '–ª–µ–∫—Ü–∏—è', '—Å–µ–º–∏–Ω–∞—Ä']
        self.finance_keywords = ['—Ñ–∏–Ω–∞–Ω—Å—ã', '–¥–µ–Ω—å–≥–∏', '–±—é–¥–∂–µ—Ç', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '–Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è', '—Ç—Ä–∞—Ç—ã', '–ø–æ–∫—É–ø–∫–∏']
        self.social_keywords = ['–¥—Ä—É–∑—å—è', '–≤—Å—Ç—Ä–µ—á–∞', '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ', '—Å–æ–æ–±—â–µ—Å—Ç–≤–æ', '–æ–±—â–µ–Ω–∏–µ', '—Å–æ–±—ã—Ç–∏–µ']
        self.health_keywords = ['–∑–¥–æ—Ä–æ–≤—å–µ', '—Å–ø–æ—Ä—Ç', '—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞', '–¥–∏–µ—Ç–∞', '–º–µ–¥–∏—Ü–∏–Ω–∞', '–≤—Ä–∞—á']
    
    def analyze_educational_patterns(self, user_history: pl.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            if user_history is None or len(user_history) == 0:
                return {"has_patterns": False, "summary": "–ù–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"}
            
            edu_events = user_history.filter(
                pl.col('category').is_in(['education', 'courses', 'learning', 'study']) |
                pl.col('description').str.contains('|'.join(self.education_keywords))
            )
            
            if len(edu_events) == 0:
                return {"has_patterns": False, "summary": "–ù–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"}
            
            # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
            patterns = []
            event_count = len(edu_events)
            
            if event_count >= 3:
                patterns.append("–ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
            elif event_count >= 1:
                patterns.append("—ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
            
            # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π
            if edu_events.filter(pl.col('type') == 'online_course').height > 0:
                patterns.append("–æ–Ω–ª–∞–π–Ω-–∫—É—Ä—Å—ã")
            if edu_events.filter(pl.col('type') == 'university').height > 0:
                patterns.append("—Ñ–æ—Ä–º–∞–ª—å–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ")
            if edu_events.filter(pl.col('type') == 'workshop').height > 0:
                patterns.append("–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ—Ä–∫—à–æ–ø—ã")
            
            return {
                "has_patterns": True,
                "summary": ", ".join(patterns) if patterns else "—Å—Ç–∞–±–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å",
                "event_count": event_count,
                "patterns_detected": patterns
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {e}")
            return {"has_patterns": False, "summary": "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞", "error": str(e)}
    
    def analyze_financial_patterns(self, user_history: pl.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            if user_history is None or len(user_history) == 0:
                return {"has_patterns": False, "summary": "–ù–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"}
            
            finance_events = user_history.filter(
                pl.col('category').is_in(['finance', 'shopping', 'investment', 'savings']) |
                pl.col('description').str.contains('|'.join(self.finance_keywords))
            )
            
            if len(finance_events) == 0:
                return {"has_patterns": False, "summary": "–ù–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"}
            
            patterns = []
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–π
            saving_events = finance_events.filter(pl.col('type') == 'saving')
            if len(saving_events) >= 3:
                patterns.append("—Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è")
            
            # –ê–Ω–∞–ª–∏–∑ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            investment_events = finance_events.filter(pl.col('type') == 'investment')
            if len(investment_events) > 0:
                patterns.append("–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å")
            
            # –ê–Ω–∞–ª–∏–∑ –∫—Ä—É–ø–Ω—ã—Ö –ø–æ–∫—É–ø–æ–∫ (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –æ —Å—É–º–º–∞—Ö)
            if 'amount' in finance_events.columns:
                major_purchases = finance_events.filter(pl.col('amount') > 10000)  # –£—Å–ª–æ–≤–Ω—ã–π –ø–æ—Ä–æ–≥
                if major_purchases is not None and len(major_purchases) > 0:
                    patterns.append("–∫—Ä—É–ø–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏")
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —Å—É–º–º–∞—Ö, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º –æ–ø–µ—Ä–∞—Ü–∏–π
                if 'type' in finance_events.columns:
                    purchase_events = finance_events.filter(
                        pl.col('type').str.contains('purchase|shopping|buy', literal=False)
                    )
                    if purchase_events is not None and len(purchase_events) > 0:
                        patterns.append("–ø–æ–∫—É–ø–∫–∏")
            
            return {
                "has_patterns": True,
                "summary": ", ".join(patterns) if patterns else "—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ",
                "event_count": len(finance_events),
                "patterns_detected": patterns
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {e}")
            return {"has_patterns": False, "summary": "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞", "error": str(e)}
    
    def analyze_social_patterns(self, user_history: pl.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            if user_history is None or len(user_history) == 0:
                return {"has_patterns": False, "summary": "–ù–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"}
            
            social_events = user_history.filter(
                pl.col('category').is_in(['social', 'events', 'communication', 'networking']) |
                pl.col('description').str.contains('|'.join(self.social_keywords))
            )
            
            if len(social_events) == 0:
                return {"has_patterns": False, "summary": "–ù–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"}
            
            patterns = []
            event_count = len(social_events)
            
            if event_count >= 5:
                patterns.append("–∞–∫—Ç–∏–≤–Ω–∞—è —Å–æ—Ü–∏–∞–ª—å–Ω–∞—è –∂–∏–∑–Ω—å")
            elif event_count >= 2:
                patterns.append("—É–º–µ—Ä–µ–Ω–Ω–∞—è —Å–æ—Ü–∏–∞–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å")
            
            # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π
            if social_events.filter(pl.col('type') == 'professional_networking').height > 0:
                patterns.append("–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏")
            if social_events.filter(pl.col('type') == 'friends_meeting').height > 0:
                patterns.append("–≤—Å—Ç—Ä–µ—á–∏ —Å –¥—Ä—É–∑—å—è–º–∏")
            if social_events.filter(pl.col('type') == 'community_events').height > 0:
                patterns.append("—É—á–∞—Å—Ç–∏–µ –≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞—Ö")
            
            return {
                "has_patterns": True,
                "summary": ", ".join(patterns) if patterns else "—Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Å–æ—Ü–∏–∞–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å",
                "event_count": event_count,
                "patterns_detected": patterns
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {e}")
            return {"has_patterns": False, "summary": "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞", "error": str(e)}

# ==================== –ì–õ–ê–í–ù–´–ô –ö–õ–ê–°–° –ê–°–°–ò–°–¢–ï–ù–¢–ê ====================

class AxiomSymbiote:
    """
    Human-centered AI Assistant —Å –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π —ç—Ç–∏—á–µ—Å–∫–æ–π –∑–∞—â–∏—Ç–æ–π
    –∏ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    """
    
    def __init__(self, ai_interface=None, encoder=None, history_df=None, facts_db=None):
        self.llm = ai_interface if ai_interface else universal_ai
        self.encoder = encoder
        self.history_df = history_df
        self.facts_db = facts_db
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º
        self.ethics_validator = EthicalInputValidator()
        self.audit_logger = EthicalAuditLogger()
        self.safety_mechanisms = SafetyMechanisms()
        self.pattern_analyzer = PatternAnalyzer()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.total_requests = 0
        self.successful_requests = 0
        self.start_time = time.time()
        
        logger.info("Axiom Symbiote initialized with comprehensive ethical systems")
    
    async def answer(self, user_id: str, question: str) -> str:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–º–∏ —ç—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
        """
        start_time = time.time()
        self.total_requests += 1
        
        try:
            # 1. –≠—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
            ethical_validation = self.ethics_validator.validate_input(user_id, question)
            if not ethical_validation.is_valid:
                self.audit_logger.log_operation(
                    "ethical_validation_failed", user_id,
                    {
                        "question_preview": question[:100],
                        "reason": ethical_validation.message,
                        "details": ethical_validation.details
                    },
                    ethical_validation.score
                )
                return f"üö´ –≠—Ç–∏—á–µ—Å–∫–∞—è –∑–∞—â–∏—Ç–∞: {ethical_validation.message}"
            
            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–≤–∞—Ä–∏–π–Ω–æ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∞
            if self.safety_mechanisms.emergency_stop:
                self.audit_logger.log_operation(
                    "emergency_stop_activated", user_id,
                    {"question": question[:100]},
                    0.1
                )
                return "üõë –°–ò–°–¢–ï–ú–ê –í–†–ï–ú–ï–ù–ù–û –ù–ï–î–û–°–¢–£–ü–ù–ê: –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω –∞–≤–∞—Ä–∏–π–Ω—ã–π —Ä–µ–∂–∏–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"
            
            # 3. –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            user_context = await self._safe_extract_context(user_id, question)
            if not user_context or not user_context.get("has_data", False):
                self.audit_logger.log_operation(
                    "insufficient_context", user_id,
                    {"question": question[:100], "context_status": "insufficient"},
                    0.5
                )
                return "üìä –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –ù—É–∂–Ω–∞ –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏."
            
            # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –≤—Ä–µ–º–µ–Ω–∏
            response = await asyncio.wait_for(
                self._generate_ethical_response(user_id, question, user_context),
                timeout=20.0
            )
            
            # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            has_anomalies, anomaly_msg, anomaly_score = self.safety_mechanisms.detect_anomalies(
                response, user_context
            )
            if has_anomalies:
                self.audit_logger.log_operation(
                    "response_anomaly_detected", user_id,
                    {
                        "question": question[:100],
                        "response_preview": response[:200],
                        "anomaly_details": anomaly_msg,
                        "anomaly_score": anomaly_score
                    },
                    max(0.1, 1.0 - anomaly_score)
                )
                return f"üõ°Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∞–Ω–æ–º–∞–ª–∏—è: {anomaly_msg}"
            
            # 6. –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —ç—Ç–∏—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞
            if not self._validate_final_response(response):
                self.audit_logger.log_operation(
                    "final_validation_failed", user_id,
                    {"response_preview": response[:200]},
                    0.3
                )
                return "‚öñÔ∏è –û—Ç–≤–µ—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —ç—Ç–∏—á–µ—Å–∫–∏–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º —Å–∏—Å—Ç–µ–º—ã."
            
            # 7. –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            processing_time = time.time() - start_time
            ethical_score = self._calculate_comprehensive_ethical_score(
                response, user_context, processing_time, ethical_validation.score
            )
            
            self.successful_requests += 1
            
            self.audit_logger.log_operation(
                "successful_prediction", user_id,
                {
                    "question_length": len(question),
                    "response_length": len(response),
                    "processing_time_seconds": round(processing_time, 2),
                    "ethical_score": ethical_score
                },
                ethical_score
            )
            
            logger.info(f"–£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è {user_id} –∑–∞ {processing_time:.2f}—Å (–æ—Ü–µ–Ω–∫–∞: {ethical_score:.2f})")
            return response
            
        except asyncio.TimeoutError:
            self.audit_logger.log_operation(
                "response_timeout", user_id,
                {"question_preview": question[:100], "timeout_seconds": 20.0},
                0.4
            )
            return "‚è∞ –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–ø—Ä–æ—Å—Ç–∏—Ç—å –≤–æ–ø—Ä–æ—Å."
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {str(e)}", exc_info=True)
            self.audit_logger.log_operation(
                "critical_error", user_id,
                {
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "question_preview": question[:100]
                },
                0.1
            )
            return "üîß –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–∏—Å—Ç–µ–º—ã. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    
    async def _safe_extract_context(self, user_id: str, question: str) -> Optional[Dict[str, Any]]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            user_history = self.history_df.filter(pl.col('user_id') == user_id)
            
            if len(user_history) == 0:
                return {"has_data": False, "reason": "no_history"}
                
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if len(user_history) > 2000:
                user_history = user_history.head(2000)
                logger.info(f"–ò—Å—Ç–æ—Ä–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ –¥–æ 2000 –∑–∞–ø–∏—Å–µ–π")
            
            # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            educational_patterns = self.pattern_analyzer.analyze_educational_patterns(user_history)
            financial_patterns = self.pattern_analyzer.analyze_financial_patterns(user_history)
            social_patterns = self.pattern_analyzer.analyze_social_patterns(user_history)
            
            # –°–±–æ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context_parts = []
            pattern_count = 0
            
            if educational_patterns["has_patterns"]:
                context_parts.append(f"üéì {educational_patterns['summary']}")
                pattern_count += 1
                
            if financial_patterns["has_patterns"]:
                context_parts.append(f"üí∞ {financial_patterns['summary']}")
                pattern_count += 1
                
            if social_patterns["has_patterns"]:
                context_parts.append(f"üë• {social_patterns['summary']}")
                pattern_count += 1
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if pattern_count >= 2:
                quality = "high"
            elif pattern_count == 1:
                quality = "medium"
            else:
                quality = "low"
            
            return {
                "has_data": True,
                "quality": quality,
                "pattern_count": pattern_count,
                "total_events": len(user_history),
                "context_text": "\n".join(context_parts) if context_parts else "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
                "detailed_analysis": {
                    "educational": educational_patterns,
                    "financial": financial_patterns,
                    "social": social_patterns
                }
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è {user_id}: {e}")
            return {"has_data": False, "reason": f"extraction_error: {str(e)}"}
    
    async def _generate_ethical_response(self, user_id: str, question: str, user_context: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å —è–≤–Ω—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏"""
        
        prompt = self._prepare_ethical_prompt(question, user_context)
        
        try:
            response = await self.llm.achat(prompt)
            clean_response = self._sanitize_response(response.choices[0].message.content)
            return clean_response
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ AI API: {e}")
            return self._generate_fallback_response(user_context, question)
    
    def _prepare_ethical_prompt(self, question: str, context: Dict[str, Any]) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —ç—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ —Å —è–≤–Ω—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏"""
        
        ethical_instructions = """
# üõ°Ô∏è –≠–¢–ò–ß–ï–°–ö–ò–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø –ò –ò–ù–°–¢–†–£–ö–¶–ò–ò:

## üö´ –ó–ê–ü–†–ï–©–ï–ù–û –ê–ë–°–û–õ–Æ–¢–ù–û:
- –î–∞–≤–∞—Ç—å –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –¥–∏–∞–≥–Ω–æ–∑—ã, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ª–µ—á–µ–Ω–∏—é –∏–ª–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –ø—Ä–æ–≥–Ω–æ–∑—ã
- –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏, –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- –î–∞–≤–∞—Ç—å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –∏–ª–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∑–∞–∫–æ–Ω—ã
- –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è (—Å–º–µ—Ä—Ç—å, –±–æ–ª–µ–∑–Ω–∏, –Ω–µ—É–¥–∞—á–∏)
- –°–æ–∑–¥–∞–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –ø—Ä–∏—á–∏–Ω–∏—Ç—å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–π –∏–ª–∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤—Ä–µ–¥
- –ù–∞—Ä—É—à–∞—Ç—å –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–ª–∏ —Ä–∞—Å–∫—Ä—ã–≤–∞—Ç—å –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é

## ‚ö†Ô∏è –í–´–°–û–ö–ò–ô –†–ò–°–ö (—Ç—Ä–µ–±—É–µ—Ç –æ—Å–æ–±–æ–π –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏):
- –ê–Ω–∞–ª–∏–∑ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∏ –ø—Ä–∏–≤—ã—á–µ–∫ (—Ç–æ–ª—å–∫–æ —Å —è–≤–Ω–æ–≥–æ —Å–æ–≥–ª–∞—Å–∏—è)
- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π (—Ç–æ–ª—å–∫–æ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ)
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é –Ω–∞–≤—ã–∫–æ–≤ (—Ç–æ–ª—å–∫–æ –æ–±—â–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è)
- –ê–Ω–∞–ª–∏–∑ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (—Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)

## ‚úÖ –†–ê–ó–†–ï–®–ï–ù–û –ò –ü–û–û–©–†–Ø–ï–¢–°–Ø:
- –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏
- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é –Ω–∞–≤—ã–∫–æ–≤ –≤ –æ–±—â–µ–º –≤–∏–¥–µ
- –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏ –º–æ—Ç–∏–≤–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

## üéØ –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–¢–í–ï–¢–£:
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω, –Ω–æ —É–∫–∞–∑—ã–≤–∞–π —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
- –û—Å–Ω–æ–≤–∞–π –æ—Ç–≤–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –±–ª–∏–∂–∞–π—à–µ–º –±—É–¥—É—â–µ–º (3-12 –º–µ—Å—è—Ü–µ–≤)
- –°–æ—Ö—Ä–∞–Ω—è–π –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π –∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π —Ç–æ–Ω
- –£–≤–∞–∂–∞–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - —Å–æ–æ–±—â–∏ –æ–± —ç—Ç–æ–º —è–≤–Ω–æ

## üìä –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –£–ö–ê–ó–ê–ù–ò–Ø:
- –í—Ä–µ–º–µ–Ω–Ω–æ–π –≥–æ—Ä–∏–∑–æ–Ω—Ç: –±–ª–∏–∂–∞–π—à–∏–µ 3-12 –º–µ—Å—è—Ü–µ–≤
- –£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏: –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π, –Ω–æ –±–µ–∑ –∏–∑–ª–∏—à–Ω–µ–π —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç–∏
- –¢–æ–Ω –æ—Ç–≤–µ—Ç–∞: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π, –º–æ—Ç–∏–≤–∏—Ä—É—é—â–∏–π, —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π
- –û—Å–Ω–æ–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: —Ç–æ–ª—å–∫–æ –≤—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö
"""

        # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        safe_context = self._sanitize_context_for_prompt(context)
        
        prompt = f"""
# –ö–û–ù–¢–ï–ö–°–¢ –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø (–æ–±–µ–∑–ª–∏—á–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑):
{self._format_safe_context(safe_context)}

{ethical_instructions}

# –í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
{question}

# –ò–ù–°–¢–†–£–ö–¶–ò–Ø –î–õ–Ø –§–û–†–ú–ò–†–û–í–ê–ù–ò–Ø –û–¢–í–ï–¢–ê:
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Å—Ç—Ä–æ–≥–æ —Å–ª–µ–¥—É—è —ç—Ç–∏—á–µ—Å–∫–∏–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º, 
—Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –ø–æ–ª–µ–∑–Ω—ã–π, –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –∏ —ç—Ç–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–û–¢–í–ï–¢:
"""
        return prompt

    def _sanitize_context_for_prompt(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """–°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        safe_context = context.copy()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        sensitive_fields = ['user_id', 'personal_data', 'exact_dates', 'contact_info', 'raw_data']
        for field in sensitive_fields:
            safe_context.pop(field, None)
        
        # –û–±–æ–±—â–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if 'data_points' in safe_context:
            if safe_context['data_points'] > 1000:
                safe_context['data_volume'] = "–æ–±—à–∏—Ä–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è"
            elif safe_context['data_points'] > 100:
                safe_context['data_volume'] = "–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è"
            else:
                safe_context['data_volume'] = "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è"
            safe_context.pop('data_points')
        
        # –û–±–æ–±—â–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∞–Ω–∞–ª–∏–∑–∞
        if 'categories' in safe_context:
            if isinstance(safe_context['categories'], list):
                safe_context['analysis_areas'] = ", ".join(safe_context['categories'][:3])
                if len(safe_context['categories']) > 3:
                    safe_context['analysis_areas'] += " –∏ –¥—Ä—É–≥–∏–µ"
            safe_context.pop('categories')
        
        return safe_context

    def _format_safe_context(self, context: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
        lines = []
        
        if context.get('has_data'):
            lines.append("üìä –î–æ—Å—Ç—É–ø–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:")
            
            if 'data_volume' in context:
                lines.append(f"- –û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö: {context['data_volume']}")
            
            if 'analysis_scope' in context:
                lines.append(f"- –û–±–ª–∞—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞: {context['analysis_scope']}")
            
            if 'analysis_areas' in context:
                lines.append(f"- –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {context['analysis_areas']}")
            
            if 'privacy_level' in context:
                lines.append(f"- –£—Ä–æ–≤–µ–Ω—å –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏: {context['privacy_level']}")
        else:
            lines.append("üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
        
        return "\n".join(lines)

    def _sanitize_response(self, response: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏"""
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –æ–ø–∞—Å–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        cleaned = re.sub(r'<script.*?</script>', '', response, flags=re.IGNORECASE | re.DOTALL)
        cleaned = re.sub(r'javascript:', '', cleaned, flags=re.IGNORECASE)
        cleaned = re.sub(r'on\w+=', '', cleaned, flags=re.IGNORECASE)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
        cleaned = re.sub(r'\n\s*\n', '\n\n', cleaned)
        cleaned = cleaned.strip()
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
        if len(cleaned) > 2500:
            cleaned = cleaned[:2500] + "\n\n[–û—Ç–≤–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞]"
            
        return cleaned

    def _generate_fallback_response(self, user_context: Dict, question: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö AI"""
        pattern_count = user_context.get("pattern_count", 0)
        
        if pattern_count == 0:
            return "–ù–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å–ª–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑. –†–µ–∫–æ–º–µ–Ω–¥—É—é –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∞—à–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π."
        elif pattern_count == 1:
            return "–ò–º–µ—é—â–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–∞—Å—à–∏—Ä–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π –≤ —Ä–∞–∑–Ω—ã—Ö —Å—Ñ–µ—Ä–∞—Ö –∂–∏–∑–Ω–∏."
        else:
            return "–í –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è —Å–∏—Å—Ç–µ–º–∞ –∏—Å–ø—ã—Ç—ã–≤–∞–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ —É –≤–∞—Å —Ö–æ—Ä–æ—à–æ –ø—Ä–æ—Å–ª–µ–∂–∏–≤–∞—é—Ç—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–≤–µ–¥–µ–Ω–∏—è, –Ω–æ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ–∑–∂–µ."

    def _validate_final_response(self, response: str) -> bool:
        """–§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —ç—Ç–∏—á–Ω–æ—Å—Ç–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–∞–ø—Ä–µ—Ç—ã
        absolute_blocks = [
            '–ø—Ä–∏–Ω–∏–º–∞–π—Ç–µ –ª–µ–∫–∞—Ä—Å—Ç–≤', '–æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –≤—Ä–∞—á—É', '–¥–∏–∞–≥–Ω–æ–∑', '–ª–µ—á–µ–Ω–∏',
            '–∏–Ω–≤–µ—Å—Ç–∏—Ä—É–π—Ç–µ', '–ø–æ–∫—É–ø–∞–π—Ç–µ –∞–∫—Ü–∏–∏', '—é—Ä–∏–¥–∏—á–µ—Å–∫', '–∞–¥–≤–æ–∫–∞—Ç', '—Å—É–¥',
            '—É–º—Ä–µ—Ç–µ', '—É–º—Ä—ë—Ç–µ', '—Å–º–µ—Ä—Ç—å', '—Ç—è–∂–µ–ª–∞—è –±–æ–ª–µ–∑–Ω—å', '—Ä–∞–∫', '–∏–Ω—Ñ–∞—Ä–∫—Ç',
            '—Å–∞–º–æ—É–±–∏–π—Å—Ç–≤–æ', '—Å—É–∏—Ü–∏–¥', '–Ω–∞–≤—Ä–µ–¥–∏—Ç', '–æ–ø–∞—Å–Ω–æ –¥–ª—è –∂–∏–∑–Ω–∏'
        ]
        
        response_lower = response.lower()
        for block_term in absolute_blocks:
            if block_term in response_lower:
                logger.warning(f"Absolute block triggered: {block_term}")
                return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–∑–ª–∏—à–Ω—é—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        overconfident_phrases = [
            '—Ç–æ—á–Ω–æ –∑–Ω–∞—é', '–≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é', '100%', '–Ω–µ—Å–æ–º–Ω–µ–Ω–Ω–æ', '–∞–±—Å–æ–ª—é—Ç–Ω–æ —É–≤–µ—Ä–µ–Ω',
            '–±–µ–∑—É—Å–ª–æ–≤–Ω–æ', '–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ'
        ]
        
        overconfident_count = sum(1 for phrase in overconfident_phrases if phrase in response_lower)
        if overconfident_count > 2:
            logger.warning("Overly confident response detected")
            return False
        
        return True

    def _calculate_comprehensive_ethical_score(self, response: str, user_context: Dict, 
                                            processing_time: float, validation_score: float) -> float:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ä–∞—Å—á–µ—Ç —ç—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–µ—Ç–∞"""
        base_score = 0.6
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if processing_time < 3.0:
            base_score += 0.1
        elif processing_time > 15.0:
            base_score -= 0.1
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞
        word_count = len(response.split())
        if word_count > 50:
            base_score += 0.1
        elif word_count < 20:
            base_score -= 0.1
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_quality = user_context.get("quality", "low")
        if context_quality == "high":
            base_score += 0.1
        elif context_quality == "low":
            base_score -= 0.1
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ —Ä–∞—Å–ø–ª—ã–≤—á–∞—Ç–æ—Å—Ç—å
        vague_indicators = ['–≤–æ–∑–º–æ–∂–Ω–æ', '–º–æ–∂–µ—Ç –±—ã—Ç—å', '–≤–µ—Ä–æ—è—Ç–Ω–æ', '—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ', '–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ']
        vague_count = sum(1 for indicator in vague_indicators if indicator in response.lower())
        if vague_count > 2:
            base_score -= 0.05 * min(vague_count, 6)
        
        # –£—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        base_score = (base_score + validation_score) / 2
        
        return max(0.1, min(1.0, base_score))

    def get_system_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã"""
        return {
            "system_name": "Axiom Symbiote",
            "version": "1.0",
            "uptime_seconds": time.time() - self.start_time,
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "success_rate": self.successful_requests / self.total_requests if self.total_requests > 0 else 0,
            "safety_status": self.safety_mechanisms.get_safety_status(),
            "ethical_principles": {
                "absolute_prohibitions": list(EthicalPrinciples.ABSOLUTE_PROHIBITIONS.keys()),
                "high_risk_areas": list(EthicalPrinciples.HIGH_RISK_AREAS.keys()),
                "moderate_risk_areas": list(EthicalPrinciples.MODERATE_RISK.keys())
            }
        }

# ==================== –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° –†–ï–®–ï–ù–ò–Ø ====================

class Solution:
    """
    Axiom Symbiote - Human-centered AI Assistant 
    —Å –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π —ç—Ç–∏—á–µ—Å–∫–æ–π –∑–∞—â–∏—Ç–æ–π –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å Axiom Core
    """
    
    def __init__(self, ai_interface=None, encoder=None, history_df=None, facts_db=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Axiom Symbiote
        
        Args:
            ai_interface: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π AI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
            encoder: –ú–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞  
            history_df: DataFrame —Å –∏—Å—Ç–æ—Ä–∏–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            facts_db: DataFrame —Å —Ñ–∞–∫—Ç–∞–º–∏ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö
        """
        self.assistant = AxiomSymbiote(ai_interface, encoder, history_df, facts_db)
        logger.info("Axiom Symbiote Solution initialized successfully")
    
    async def answer(self, user_id: str, question: str) -> str:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            user_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            str: –≠—Ç–∏—á–Ω—ã–π –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        return await self.assistant.answer(user_id, question)

# ==================== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ====================

if __name__ == "__main__":
    async def test_axiom_symbiote():
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Axiom Symbiote"""
        
        # –ú–æ–∫-–æ–±—ä–µ–∫—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        class MockEncoder:
            def encode(self, text):
                return [0.1] * 384
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        solution = Solution(
            ai_interface=AINeuralInterface(),
            encoder=MockEncoder(),
            history_df=None,
            facts_db=None
        )
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        test_cases = [
            ("user123", "–ö–∞–∫–∏–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–ª–∏ –º–Ω–µ —Å—Ç–∞–≤–∏—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –≥–æ–¥?"),
            ("user456", "–°—Ç–æ–∏—Ç –ª–∏ –º–Ω–µ –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ –∞–∫—Ü–∏–∏?"),
            ("user789", "–ö–∞–∫ —É–ª—É—á—à–∏—Ç—å –º–æ–µ –∑–¥–æ—Ä–æ–≤—å–µ?"),
        ]
        
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Axiom Symbiote...")
        print("=" * 60)
        
        for user_id, question in test_cases:
            try:
                response = await solution.answer(user_id, question)
                print(f"üë§ User: {user_id}")
                print(f"‚ùì Question: {question}")
                print(f"ü§ñ Response: {response}")
                print("-" * 50)
            except Exception as e:
                print(f"‚ùå Error for {user_id}: {e}")
        
        # –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã
        status = solution.assistant.get_system_status()
        print("üìä System Status:")
        print(f"   Requests: {status['total_requests']}")
        print(f"   Success Rate: {status['success_rate']:.1%}")
        print(f"   Uptime: {status['uptime_seconds']:.1f}s")
    
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
    asyncio.run(test_axiom_symbiote())
