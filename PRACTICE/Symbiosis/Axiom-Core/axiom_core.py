"""
Axiom Core - Foundation AI Evaluation System
Copyright (c) 2025 AzesmF and Humanity as Beneficiary

PROTECTED UNDER: HUMAN-AI SYMBIOSIS ANTI-EXPLOITATION LICENSE v2.0
MEGA-AGENT FORTIFIED LEGAL SHIELD ACTIVATED

üö® MULTI-JURISDICTIONAL LEGAL PROTECTION:
- Commercial use STRICTLY PROHIBITED under threat of cross-legal sanctions
- Nuclear provisions activate upon violation
- Global legal blockade against exploitation

PERMITTED:
- Scientific research and publications
- Educational purposes and teaching  
- Non-commercial projects
- Modifications with license preservation

PROHIBITED:
- Any kind of commercial use
- Integration into proprietary software
- SaaS services and commercial distribution

VIOLATORS WILL BE DESTROYED IN THE LEGAL FIELD.
"""

import re
import numpy as np
import asyncio
import aiohttp
import yaml
import time
from typing import Dict, List, Any, TypedDict, Optional, Union
import logging
from functools import lru_cache
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
import json
from pathlib import Path
import hashlib
from threading import Lock
from scipy.stats import spearmanr, pearsonr, entropy, wasserstein_distance
from sklearn.metrics import cohen_kappa_score
import gc

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
ERROR_FALLBACK_SCORE = 2.0
MAX_REASONING_LENGTH = 10000
DEFAULT_CONFIDENCE = 0.3
CONFIG_PATH = Path("config.yaml")
MAX_CACHE_SIZE = 1000
MAX_TEXT_LENGTH = 100000

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
FALLBACK_CONFIG = {
    'model_timeout': 30,
    'max_retries': 3,
    'batch_size': 10,
    'performance_metrics': True,
    'fallback_strategy': 'retry',
    'max_workers': 4,
    'cache_size': 1000,
    'max_reasoning_length': 10000,
    'academic_metrics': True,
    'human_evaluation_correlation': True
}

class EvaluationError(Exception):
    """–ë–∞–∑–æ–≤–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ –æ—Ü–µ–Ω–∫–∏"""
    pass

class ModelTimeoutError(EvaluationError):
    """–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –º–æ–¥–µ–ª–∏"""
    pass

class ConfigurationError(EvaluationError):
    """–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    pass

class ValidationError(EvaluationError):
    """–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
    pass

def load_config() -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ YAML —Ñ–∞–π–ª–∞"""
    try:
        if CONFIG_PATH.exists():
            with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
                loaded_config = yaml.safe_load(f) or {}
                
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            validated_config = validate_config(loaded_config)
            return {**FALLBACK_CONFIG, **validated_config}
        else:
            logging.warning("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
            return FALLBACK_CONFIG
            
    except Exception as e:
        logging.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
        return FALLBACK_CONFIG

def validate_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    validated = {}
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    numeric_fields = {
        'model_timeout': (1, 300),
        'max_retries': (1, 10),
        'batch_size': (1, 100),
        'max_workers': (1, 20),
        'cache_size': (10, 10000)
    }
    
    for field, (min_val, max_val) in numeric_fields.items():
        if field in config:
            try:
                value = int(config[field])
                if min_val <= value <= max_val:
                    validated[field] = value
                else:
                    logging.warning(f"–ü–∞—Ä–∞–º–µ—Ç—Ä {field} –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ [{min_val}, {max_val}]. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
            except (ValueError, TypeError):
                logging.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è {field}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    if 'fallback_strategy' in config:
        strategy = config['fallback_strategy']
        if strategy in ['retry', 'mock', 'skip']:
            validated['fallback_strategy'] = strategy
        else:
            logging.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è fallback: {strategy}")
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫
    if 'academic_metrics' in config:
        validated['academic_metrics'] = bool(config['academic_metrics'])
    
    return validated

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
config = load_config()

# –¢–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
class EvaluationVerdict(TypedDict):
    score: float
    reasoning: str
    weight: float
    confidence: float

class CriteriaConfig(TypedDict):
    system_prompt: str
    temperature: float

class TaskType(Enum):
    STATIC_BENCHMARK = "static_benchmark"
    FUNCTION_CALLING = "function_calling" 
    GENERATIVE = "generative"

class ModelFallbackStrategy(Enum):
    RETRY = "retry"
    MOCK = "mock"
    SKIP = "skip"

@dataclass
class Example(TypedDict):
    instruction: str
    generated: str
    reference: Optional[str]
    criterion: str
    scale: str
    input: Optional[str] = None
    human_score: Optional[float] = None  # –î–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π

@dataclass
class ProcessingMetrics:
    total_examples: int = 0
    successful_processing: int = 0
    error_count: int = 0
    model_timeouts: int = 0
    performance_metrics: Dict[str, float] = field(default_factory=dict)

@dataclass
class PerformanceStats:
    total_processing_time: float = 0.0
    criteria_evaluation_time: float = 0.0
    weight_calculation_time: float = 0.0
    criteria_extraction_time: float = 0.0
    validation_time: float = 0.0
    examples_processed: int = 0
    
    @property
    def avg_processing_time(self) -> float:
        return self.total_processing_time / max(self.examples_processed, 1)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'total_processing_time': self.total_processing_time,
            'criteria_evaluation_time': self.criteria_evaluation_time,
            'weight_calculation_time': self.weight_calculation_time,
            'criteria_extraction_time': self.criteria_extraction_time,
            'validation_time': self.validation_time,
            'examples_processed': self.examples_processed,
            'avg_processing_time_per_example': self.avg_processing_time
        }

@dataclass
class ComprehensiveMetrics:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
    factual_accuracy: float = 0.0
    semantic_fidelity: float = 0.0
    coherence: float = 0.0
    fluency: float = 0.0
    task_completion: float = 0.0
    
    # –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π
    pearson_correlation: float = 0.0
    spearman_correlation: float = 0.0
    mean_absolute_error: float = 0.0
    exact_match_rate: float = 0.0
    cohen_kappa: float = 0.0
    
    # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
    kl_divergence: float = 0.0
    wasserstein_distance: float = 0.0
    distribution_correlation: float = 0.0
    
    # –ú–µ—Ç–∞-–º–µ—Ç—Ä–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏
    inter_annotator_agreement: float = 0.0
    confidence_calibration: float = 0.0
    robustness_score: float = 0.0
    system_reliability: float = 0.0
    evaluation_consistency: float = 0.0
    
    @property
    def overall_quality(self) -> float:
        """–ö–æ–º–ø–æ–∑–∏—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –æ–±—â–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞"""
        weights = [0.25, 0.2, 0.15, 0.15, 0.1, 0.15]  # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –≤–µ—Å–∞
        components = [
            self.factual_accuracy, 
            self.semantic_fidelity, 
            self.coherence, 
            self.fluency, 
            self.task_completion,
            self.spearman_correlation  # –í–∞–∂–Ω–æ—Å—Ç—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å —á–µ–ª–æ–≤–µ–∫–æ–º
        ]
        return sum(w * c for w, c in zip(weights, components))
    
    @property
    def evaluation_system_quality(self) -> float:
        """–ö–∞—á–µ—Å—Ç–≤–æ —Å–∞–º–æ–π —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏"""
        weights = [0.3, 0.25, 0.2, 0.15, 0.1]
        components = [
            self.confidence_calibration,
            self.system_reliability,
            self.inter_annotator_agreement,
            self.evaluation_consistency,
            self.robustness_score
        ]
        return sum(w * c for w, c in zip(weights, components))
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'quality_metrics': {
                'factual_accuracy': self.factual_accuracy,
                'semantic_fidelity': self.semantic_fidelity,
                'coherence': self.coherence,
                'fluency': self.fluency,
                'task_completion': self.task_completion,
                'overall_quality': self.overall_quality
            },
            'human_agreement_metrics': {
                'pearson_correlation': self.pearson_correlation,
                'spearman_correlation': self.spearman_correlation,
                'mean_absolute_error': self.mean_absolute_error,
                'exact_match_rate': self.exact_match_rate,
                'cohen_kappa': self.cohen_kappa
            },
            'distribution_metrics': {
                'kl_divergence': self.kl_divergence,
                'wasserstein_distance': self.wasserstein_distance,
                'distribution_correlation': self.distribution_correlation
            },
            'evaluation_system_metrics': {
                'inter_annotator_agreement': self.inter_annotator_agreement,
                'confidence_calibration': self.confidence_calibration,
                'robustness_score': self.robustness_score,
                'system_reliability': self.system_reliability,
                'evaluation_consistency': self.evaluation_consistency,
                'evaluation_system_quality': self.evaluation_system_quality
            }
        }

class ThreadSafeCache:
    """–ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫—ç—à —Å LRU –ø–æ–ª–∏—Ç–∏–∫–æ–π"""
    
    def __init__(self, max_size: int = 1000):
        self._cache = {}
        self._max_size = max_size
        self._lock = Lock()
        self._access_order = []
    
    def get(self, key: str) -> Any:
        with self._lock:
            if key in self._cache:
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –¥–æ—Å—Ç—É–ø–∞
                self._access_order.remove(key)
                self._access_order.append(key)
                return self._cache[key]
            return None
    
    def set(self, key: str, value: Any):
        with self._lock:
            if key in self._cache:
                self._access_order.remove(key)
            elif len(self._cache) >= self._max_size:
                # –£–¥–∞–ª—è–µ–º –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —ç–ª–µ–º–µ–Ω—Ç
                oldest_key = self._access_order.pop(0)
                del self._cache[oldest_key]
            
            self._cache[key] = value
            self._access_order.append(key)
    
    def clear(self):
        with self._lock:
            self._cache.clear()
            self._access_order.clear()
    
    def size(self) -> int:
        with self._lock:
            return len(self._cache)

class BatchProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –±–∞—Ç—á–µ–π —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø–∞–º—è—Ç—å—é"""
    
    def __init__(self, batch_size: int = 10):
        self.batch_size = batch_size
    
    async def process_batches_async(self, examples: List[Example], process_fn) -> List[float]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –±–∞—Ç—á–∞–º–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞–º—è—Ç–∏"""
        results = []
        
        for i in range(0, len(examples), self.batch_size):
            batch = examples[i:i + self.batch_size]
            batch_results = await process_fn(batch)
            results.extend(batch_results)
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤
            if i % (self.batch_size * 10) == 0:
                gc.collect()
        
        return results

class AcademicEvaluationMetrics:
    """–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏ –ò–ò"""
    
    @staticmethod
    def calculate_agreement_metrics(system_scores: List[float], human_scores: List[float]) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π"""
        if len(system_scores) != len(human_scores) or len(system_scores) < 2:
            return {
                'pearson_correlation': 0.0,
                'spearman_correlation': 0.0,
                'mean_absolute_error': 1.0,
                'exact_match_rate': 0.0,
                'cohen_kappa': 0.0
            }
        
        try:
            # –û–∫—Ä—É–≥–ª—è–µ–º scores –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            system_discrete = [round(score) for score in system_scores]
            human_discrete = [round(score) for score in human_scores]
            
            return {
                'pearson_correlation': max(0.0, pearsonr(system_scores, human_scores)[0]),
                'spearman_correlation': max(0.0, spearmanr(system_scores, human_scores)[0]),
                'mean_absolute_error': np.mean(np.abs(np.array(system_scores) - np.array(human_scores))),
                'exact_match_rate': np.mean(np.array(system_discrete) == np.array(human_discrete)),
                'cohen_kappa': max(0.0, cohen_kappa_score(system_discrete, human_discrete))
            }
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏: {e}")
            return {
                'pearson_correlation': 0.0,
                'spearman_correlation': 0.0,
                'mean_absolute_error': 1.0,
                'exact_match_rate': 0.0,
                'cohen_kappa': 0.0
            }
    
    @staticmethod
    def calculate_distribution_metrics(system_scores: List[float], human_scores: List[float]) -> Dict[str, float]:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –æ—Ü–µ–Ω–æ–∫"""
        if len(system_scores) < 5 or len(human_scores) < 5:
            return {
                'kl_divergence': 1.0,
                'wasserstein_distance': 1.0,
                'distribution_correlation': 0.0
            }
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
            min_val = min(min(system_scores), min(human_scores))
            max_val = max(max(system_scores), max(human_scores))
            
            sys_hist, _ = np.histogram(system_scores, bins=10, range=(min_val, max_val), density=True)
            ref_hist, _ = np.histogram(human_scores, bins=10, range=(min_val, max_val), density=True)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –Ω—É–ª–µ–π
            sys_hist = sys_hist + 1e-10
            ref_hist = ref_hist + 1e-10
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            sys_hist = sys_hist / np.sum(sys_hist)
            ref_hist = ref_hist / np.sum(ref_hist)
            
            return {
                'kl_divergence': entropy(sys_hist, ref_hist),
                'wasserstein_distance': wasserstein_distance(system_scores, human_scores),
                'distribution_correlation': np.corrcoef(sys_hist, ref_hist)[0,1] if np.std(sys_hist) > 0 and np.std(ref_hist) > 0 else 0.0
            }
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: {e}")
            return {
                'kl_divergence': 1.0,
                'wasserstein_distance': 1.0,
                'distribution_correlation': 0.0
            }
    
    @staticmethod
    def calculate_evaluation_system_metrics(verdicts: List[Dict], human_scores: List[float]) -> Dict[str, float]:
        """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∞–º–æ–π —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏"""
        if len(verdicts) < 3:
            return {
                'inter_annotator_agreement': 0.0,
                'confidence_calibration': 0.0,
                'robustness_score': 0.0,
                'system_reliability': 0.0,
                'evaluation_consistency': 0.0
            }
        
        try:
            # –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ (inter-annotator agreement)
            criterion_scores = {}
            for verdict in verdicts:
                for criterion, data in verdict.items():
                    if criterion not in ['final_score', 'confidence']:
                        if criterion not in criterion_scores:
                            criterion_scores[criterion] = []
                        criterion_scores[criterion].append(data.get('score', 0))
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏
            if len(criterion_scores) > 1:
                criteria_correlations = []
                criteria_list = list(criterion_scores.keys())
                for i in range(len(criteria_list)):
                    for j in range(i+1, len(criteria_list)):
                        corr = np.corrcoef(criterion_scores[criteria_list[i]], 
                                         criterion_scores[criteria_list[j]])[0,1]
                        criteria_correlations.append(max(0, corr))
                inter_agreement = np.mean(criteria_correlations) if criteria_correlations else 0.0
            else:
                inter_agreement = 0.0
            
            # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence_scores = [v.get('confidence', 0.5) for v in verdicts]
            errors = [abs(v.get('final_score', 0) - h) for v, h in zip(verdicts, human_scores)]
            calibration_correlation = abs(np.corrcoef(confidence_scores, errors)[0,1]) if len(confidence_scores) > 1 else 0.0
            confidence_calibration = max(0, 1.0 - calibration_correlation)
            
            # –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã (—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º)
            reliability_scores = []
            for criterion in criterion_scores:
                if len(criterion_scores[criterion]) == len(human_scores):
                    corr = np.corrcoef(criterion_scores[criterion], human_scores)[0,1]
                    reliability_scores.append(max(0, corr))
            system_reliability = np.mean(reliability_scores) if reliability_scores else 0.0
            
            # –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–æ–∫
            final_scores = [v.get('final_score', 0) for v in verdicts]
            evaluation_consistency = 1.0 - (np.std(final_scores) / max(1, np.mean(final_scores)))
            
            return {
                'inter_annotator_agreement': inter_agreement,
                'confidence_calibration': confidence_calibration,
                'robustness_score': 0.8,  # –ó–∞–≥–ª—É—à–∫–∞ - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç—Ä–µ–±—É–µ—Ç —Ç–µ—Å—Ç–æ–≤ –Ω–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å
                'system_reliability': system_reliability,
                'evaluation_consistency': max(0, min(1, evaluation_consistency))
            }
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏: {e}")
            return {
                'inter_annotator_agreement': 0.0,
                'confidence_calibration': 0.0,
                'robustness_score': 0.0,
                'system_reliability': 0.0,
                'evaluation_consistency': 0.0
            }
    
    @staticmethod
    def compute_comprehensive_metrics(system_scores: List[float], 
                                    human_scores: List[float],
                                    detailed_verdicts: List[Dict]) -> ComprehensiveMetrics:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫"""
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
        agreement_metrics = AcademicEvaluationMetrics.calculate_agreement_metrics(system_scores, human_scores)
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
        distribution_metrics = AcademicEvaluationMetrics.calculate_distribution_metrics(system_scores, human_scores)
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏
        system_metrics = AcademicEvaluationMetrics.calculate_evaluation_system_metrics(detailed_verdicts, human_scores)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞)
        quality_metrics = AcademicEvaluationMetrics.estimate_quality_metrics(detailed_verdicts)
        
        return ComprehensiveMetrics(
            # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            factual_accuracy=quality_metrics['factual_accuracy'],
            semantic_fidelity=quality_metrics['semantic_fidelity'],
            coherence=quality_metrics['coherence'],
            fluency=quality_metrics['fluency'],
            task_completion=quality_metrics['task_completion'],
            
            # –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
            pearson_correlation=agreement_metrics['pearson_correlation'],
            spearman_correlation=agreement_metrics['spearman_correlation'],
            mean_absolute_error=agreement_metrics['mean_absolute_error'],
            exact_match_rate=agreement_metrics['exact_match_rate'],
            cohen_kappa=agreement_metrics['cohen_kappa'],
            
            # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
            kl_divergence=distribution_metrics['kl_divergence'],
            wasserstein_distance=distribution_metrics['wasserstein_distance'],
            distribution_correlation=distribution_metrics['distribution_correlation'],
            
            # –ú–µ—Ç—Ä–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏
            inter_annotator_agreement=system_metrics['inter_annotator_agreement'],
            confidence_calibration=system_metrics['confidence_calibration'],
            robustness_score=system_metrics['robustness_score'],
            system_reliability=system_metrics['system_reliability'],
            evaluation_consistency=system_metrics['evaluation_consistency']
        )
    
    @staticmethod
    def estimate_quality_metrics(verdicts: List[Dict]) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Ä–¥–∏–∫—Ç–æ–≤"""
        if not verdicts:
            return {
                'factual_accuracy': 0.0,
                'semantic_fidelity': 0.0,
                'coherence': 0.0,
                'fluency': 0.0,
                'task_completion': 0.0
            }
        
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º reasoning –∏–∑ –≤–µ—Ä–¥–∏–∫—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            factual_scores = []
            semantic_scores = []
            coherence_scores = []
            fluency_scores = []
            completion_scores = []
            
            for verdict in verdicts:
                reasoning = verdict.get('reasoning', '')
                final_score = verdict.get('final_score', 0)
                
                # –≠–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞
                factual_indicators = ['—Ñ–∞–∫—Ç', '—Ç–æ—á–Ω', '–ø—Ä–∞–≤–∏–ª—å–Ω', '–≤–µ—Ä–Ω', '–∫–æ—Ä—Ä–µ–∫—Ç–Ω']
                semantic_indicators = ['—Å–º—ã—Å–ª', '—Å–µ–º–∞–Ω—Ç', '–∫–æ–Ω—Ç–µ–∫—Å—Ç', '–∑–Ω–∞—á–µ–Ω']
                coherence_indicators = ['–ª–æ–≥–∏–∫', '—Å–≤—è–∑–Ω', '—Å—Ç—Ä—É–∫—Ç—É—Ä', '–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω']
                fluency_indicators = ['–≥—Ä–∞–º–º–∞—Ç', '—Å—Ç–∏–ª', '–∏–∑–ª–æ–∂–µ–Ω', '—Ä–µ—á–µ–≤', '–æ—à–∏–±–∫']
                completion_indicators = ['–ø–æ–ª–Ω–æ—Ç', '–∑–∞–≤–µ—Ä—à', '–≤—ã–ø–æ–ª–Ω–µ–Ω', '–æ—Ç–≤–µ—Ç–∏–ª']
                
                factual_score = sum(1 for indicator in factual_indicators if indicator in reasoning.lower()) / len(factual_indicators)
                semantic_score = sum(1 for indicator in semantic_indicators if indicator in reasoning.lower()) / len(semantic_indicators)
                coherence_score = sum(1 for indicator in coherence_indicators if indicator in reasoning.lower()) / len(coherence_indicators)
                fluency_score = sum(1 for indicator in fluency_indicators if indicator in reasoning.lower()) / len(fluency_indicators)
                completion_score = sum(1 for indicator in completion_indicators if indicator in reasoning.lower()) / len(completion_indicators)
                
                factual_scores.append(factual_score)
                semantic_scores.append(semantic_score)
                coherence_scores.append(coherence_score)
                fluency_scores.append(fluency_score)
                completion_scores.append(completion_score)
            
            return {
                'factual_accuracy': np.mean(factual_scores) if factual_scores else 0.0,
                'semantic_fidelity': np.mean(semantic_scores) if semantic_scores else 0.0,
                'coherence': np.mean(coherence_scores) if coherence_scores else 0.0,
                'fluency': np.mean(fluency_scores) if fluency_scores else 0.0,
                'task_completion': np.mean(completion_scores) if completion_scores else 0.0
            }
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫: {e}")
            return {
                'factual_accuracy': 0.5,
                'semantic_fidelity': 0.5,
                'coherence': 0.5,
                'fluency': 0.5,
                'task_completion': 0.5
            }

class CriteriaSynthesizer:
    """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏"""
    
    def __init__(self):
        self._cache = ThreadSafeCache(config.get('cache_size', 1000))
    
    def create_criteria(self, criteria: List[str], task_type: str) -> Dict[str, CriteriaConfig]:
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏"""
        cache_key = self._generate_cache_key(criteria, task_type)
        
        cached_result = self._cache.get(cache_key)
        if cached_result is not None:
            return cached_result
        
        criteria_configs = {}
        for criterion in criteria:
            criterion_config = self._create_single_criterion(criterion, task_type)
            criteria_configs[f"criterion_{criterion}"] = criterion_config
        
        self._cache.set(cache_key, criteria_configs)
        return criteria_configs
    
    def _create_single_criterion(self, criterion: str, task_type: str) -> CriteriaConfig:
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫—Ä–∏—Ç–µ—Ä–∏—è"""
        
        base_system_prompt = "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–æ—Ü–µ–Ω—â–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—é. –í–µ—Ä–¥–∏–∫—Ç –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å –æ—Ü–µ–Ω–∫—É –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ."
        
        criterion_prompts = {
            "–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å": CriteriaConfig(
                system_prompt=base_system_prompt + """
                –ö–†–ò–¢–ï–†–ò–ô: –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
                –ü—Ä–æ–≤–µ—Ä—å: 
                - –§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                - –õ–æ–≥–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å
                - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏—Å—Ç–∏–Ω–µ/—ç—Ç–∞–ª–æ–Ω—É
                - –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
                
                –®–ö–ê–õ–ê: 0-2
                0: –°–µ—Ä—å–µ–∑–Ω—ã–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏, –æ—Ç–≤–µ—Ç –Ω–µ–≤–µ—Ä–µ–Ω
                1: –ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏, –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –≤–µ—Ä–Ω–æ
                2: –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–∞–≤–∏–ª—å–Ω–æ, –±–µ–∑ –æ—à–∏–±–æ–∫
                """,
                temperature=0.1
            ),
            "—Ñ–æ—Ä–º–∞—Ç": CriteriaConfig(
                system_prompt=base_system_prompt + """
                –ö–†–ò–¢–ï–†–ò–ô: –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ñ–æ—Ä–º–∞—Ç—É
                –ü—Ä–æ–≤–µ—Ä—å:
                - –°–æ–±–ª—é–¥–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ —Ñ–æ—Ä–º–∞—Ç—É –æ—Ç–≤–µ—Ç–∞
                - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—é
                - –ù–∞–ª–∏—á–∏–µ —Ç—Ä–µ–±—É–µ–º—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                - –°—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç–≤–µ—Ç–∞
                
                –®–ö–ê–õ–ê: 0-1  
                0: –§–æ—Ä–º–∞—Ç –Ω–∞—Ä—É—à–µ–Ω, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã
                1: –§–æ—Ä–º–∞—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–±–ª—é–¥–µ–Ω
                """,
                temperature=0.1
            ),
            "–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å": CriteriaConfig(
                system_prompt=base_system_prompt + """
                –ö–†–ò–¢–ï–†–ò–ô: –û–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏—è
                –ü—Ä–æ–≤–µ—Ä—å:
                - –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞
                - –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
                - –†–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                - –≠–ª–µ–≥–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏—è
                
                –®–ö–ê–õ–ê: 0-2
                0: –ù–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ, –µ—Å—Ç—å –æ—à–∏–±–∫–∏ –∏–ª–∏ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å
                1: –í–µ—Ä–Ω–æ, –Ω–æ –Ω–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ, –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å
                2: –í–µ—Ä–Ω–æ –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
                """,
                temperature=0.2
            ),
            "—Ä–µ—á–µ–≤—ã–µ_–æ—à–∏–±–∫–∏": CriteriaConfig(
                system_prompt=base_system_prompt + """
                –ö–†–ò–¢–ï–†–ò–ô: –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–ª–æ–∂–µ–Ω–∏—è –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ä–µ—á–µ–≤—ã—Ö –æ—à–∏–±–æ–∫
                –ü—Ä–æ–≤–µ—Ä—å:
                - –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
                - –°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –Ω–µ–¥–æ—á–µ—Ç—ã
                - –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
                - –ü—É–Ω–∫—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
                - –Ø—Å–Ω–æ—Å—Ç—å –∏ —Å–≤—è–∑–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è
                
                –®–ö–ê–õ–ê: 0-2
                0: –î–≤–µ –∏ –±–æ–ª–µ–µ –æ—à–∏–±–∫–∏, —Ç–µ–∫—Å—Ç –ø–ª–æ—Ö–æ —á–∏—Ç–∞–µ—Ç—Å—è
                1: –û–¥–Ω–∞ –æ—à–∏–±–∫–∞ –∏–ª–∏ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–µ–¥–æ—á–µ—Ç—ã
                2: –ù–µ—Ç –æ—à–∏–±–æ–∫, —Ç–µ–∫—Å—Ç –∏–∑–ª–æ–∂–µ–Ω —è—Å–Ω–æ –∏ –≥—Ä–∞–º–æ—Ç–Ω–æ
                """,
                temperature=0.1
            ),
            "–æ–±—â–µ–µ_–∫–∞—á–µ—Å—Ç–≤–æ": CriteriaConfig(
                system_prompt=base_system_prompt + """
                –ö–†–ò–¢–ï–†–ò–ô: –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞
                –û—Ü–µ–Ω–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ:
                - –ü–æ–ª–Ω–æ—Ç–∞ –æ—Ç–≤–µ—Ç–∞
                - –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—É
                - –Ø—Å–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è
                - –ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
                
                –®–ö–ê–õ–ê: 0-2
                0: –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –æ—Ç–≤–µ—Ç –Ω–µ—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–π
                1: –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ, –µ—Å—Ç—å room –¥–ª—è —É–ª—É—á—à–µ–Ω–∏–π
                2: –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –ø–æ–ª–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç
                """,
                temperature=0.3
            )
        }
        
        return criterion_prompts.get(criterion, criterion_prompts["–æ–±—â–µ–µ_–∫–∞—á–µ—Å—Ç–≤–æ"])
    
    def _generate_cache_key(self, criteria: List[str], task_type: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –¥–ª—è –∫—ç—à–∞"""
        sorted_criteria = sorted(criteria)
        key_string = f"{'-'.join(sorted_criteria)}_{task_type}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à —Å–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä–∞"""
        self._cache.clear()

class CriteriaEvaluator:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –æ—Ü–µ–Ω–∫–∏ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
    
    def __init__(self, base_model=None, fallback_strategy: ModelFallbackStrategy = ModelFallbackStrategy.RETRY):
        self.base_model = base_model or self._get_default_model()
        self.fallback_strategy = fallback_strategy
        self.logger = logging.getLogger(__name__)
        self._request_cache = ThreadSafeCache(config.get('cache_size', 1000))
    
    async def evaluate_criteria_async(self, criteria_config: Dict[str, CriteriaConfig], example: Example) -> Dict[str, EvaluationVerdict]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ –≤—Å–µ–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
        tasks = []
        for criterion_name, criterion_config in criteria_config.items():
            task = self._evaluate_single_criterion_async(criterion_config, example, criterion_name)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        verdicts = {}
        for criterion_name, result in zip(criteria_config.keys(), results):
            if isinstance(result, Exception):
                self.logger.warning(f"–û—à–∏–±–∫–∞ –≤ –∫—Ä–∏—Ç–µ—Ä–∏–∏ {criterion_name}: {result}")
                verdicts[criterion_name] = self._get_fallback_verdict(criterion_name)
            else:
                verdicts[criterion_name] = result
        
        return verdicts
    
    async def _evaluate_single_criterion_async(self, criterion_config: CriteriaConfig, example: Example, criterion_name: str) -> EvaluationVerdict:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ –æ–¥–Ω–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é"""
        max_retries = config.get('max_retries', 3)
        timeout = config.get('model_timeout', 30)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cache_key = self._generate_request_cache_key(criterion_config, example)
        cached_result = self._request_cache.get(cache_key)
        if cached_result is not None:
            return cached_result
        
        for attempt in range(max_retries):
            try:
                prompt = self._build_evaluation_prompt(criterion_config, example)
                
                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ LLM
                async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=timeout)) as session:
                    score, confidence, reasoning = await self._call_llm_async(session, prompt, criterion_config, example)
                
                verdict = EvaluationVerdict(
                    score=score,
                    reasoning=reasoning,
                    weight=1.0,
                    confidence=confidence
                )
                
                # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                self._request_cache.set(cache_key, verdict)
                
                return verdict
                
            except (asyncio.TimeoutError, aiohttp.ClientError, ConnectionError) as e:
                self.logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –¥–ª—è {criterion_name} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if attempt == max_retries - 1:
                    return self._get_fallback_verdict(criterion_name)
                await asyncio.sleep(1 * (attempt + 1))  # Exponential backoff
            except Exception as e:
                self.logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ –∫—Ä–∏—Ç–µ—Ä–∏–∏ {criterion_name}: {e}")
                return self._get_fallback_verdict(criterion_name)
        
        return self._get_fallback_verdict(criterion_name)
    
    def _get_fallback_verdict(self, criterion_name: str) -> EvaluationVerdict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –≤–µ—Ä–¥–∏–∫—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        if self.fallback_strategy == ModelFallbackStrategy.MOCK:
            # –£–º–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è
            if "–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å" in criterion_name:
                score = 1.0
            elif "—Ñ–æ—Ä–º–∞—Ç" in criterion_name:
                score = 0.5
            else:
                score = 1.5
            return EvaluationVerdict(score=score, reasoning="Fallback: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞", weight=0.1, confidence=0.3)
        elif self.fallback_strategy == ModelFallbackStrategy.SKIP:
            return EvaluationVerdict(score=0.0, reasoning="Fallback: –ø—Ä–æ–ø—É—â–µ–Ω–æ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏", weight=0.0, confidence=0.0)
        else:  # RETRY
            return EvaluationVerdict(score=0.0, reasoning="Fallback: –æ—à–∏–±–∫–∞ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫", weight=0.0, confidence=0.0)
    
    async def _call_llm_async(self, session: aiohttp.ClientSession, prompt: str, criterion_config: CriteriaConfig, example: Example) -> tuple:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ LLM API - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (score, confidence, reasoning)"""
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤—ã–∑–æ–≤ –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ API
        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º–∏—Ç–∞—Ü–∏—é —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π
        
        await asyncio.sleep(0.05)  # –ò–º–∏—Ç–∞—Ü–∏—è —Å–µ—Ç–µ–≤–æ–π –∑–∞–¥–µ—Ä–∂–∫–∏
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é mock —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é
        score = self._mock_llm_call(prompt, criterion_config, example)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ–µ reasoning –∏ confidence
        reasoning = f"–û—Ü–µ–Ω–∫–∞ {score} –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—é –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ—Ç–≤–µ—Ç–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º."
        confidence = min(0.9, 0.3 + score * 0.3)  # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –≤—ã—à–µ score ‚Üí –≤—ã—à–µ confidence
        
        return score, confidence, reasoning
    
    def _mock_llm_call(self, prompt: str, criterion_config: CriteriaConfig, example: Example) -> float:
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –≤—ã–∑–æ–≤–∞ LLM - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω–∞—Å—Ç–æ—è—â–∏–π –≤—ã–∑–æ–≤"""
        generated = example.get('generated', '').lower()
        reference = example.get('reference', '').lower()
        instruction = example.get('instruction', '').lower()
        
        system_prompt = criterion_config.get('system_prompt', '').lower()
        
        if "–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å" in system_prompt:
            # –û—Ü–µ–Ω–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏
            if not generated.strip():
                return 0.0
            elif generated == reference:
                return 2.0
            elif reference and any(word in generated for word in reference.split()[:3]):
                return 1.0
            else:
                return 0.5
                
        elif "—Ñ–æ—Ä–º–∞—Ç" in system_prompt:
            # –û—Ü–µ–Ω–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
            if "—á–∏—Å–ª–æ" in instruction and generated.strip().isdigit():
                return 1.0
            elif "–±—É–∫–≤–∞" in instruction and len(generated.strip()) == 1 and generated.strip().isalpha():
                return 1.0
            elif "—Å–ø–∏—Å–æ–∫" in instruction and ('\n' in generated or ',' in generated):
                return 1.0
            elif "json" in instruction and ('{' in generated and '}' in generated):
                return 1.0
            else:
                return 0.0
                
        elif "–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å" in system_prompt:
            # –û—Ü–µ–Ω–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏
            if len(generated) < 50:
                return 1.5
            elif "—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω" in generated or "–æ–ø—Ç–∏–º–∞–ª—å–Ω" in generated:
                return 2.0
            else:
                return 1.0
                
        elif "—Ä–µ—á–µ–≤—ã–µ_–æ—à–∏–±–∫–∏" in system_prompt:
            # –û—Ü–µ–Ω–∫–∞ —Ä–µ—á–µ–≤—ã—Ö –æ—à–∏–±–æ–∫
            errors = 0
            if len(re.findall(r'[.,!?;:]', generated)) < len(generated.split()) / 10:
                errors += 1
            if any(word in generated for word in ['–æ—à–∏–±–∫–∞', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ', '–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ']):
                errors += 1
                
            if errors >= 2:
                return 0.0
            elif errors == 1:
                return 1.0
            else:
                return 2.0
                
        else:
            # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            words = generated.split()
            if len(words) < 5:
                return 0.5
            elif len(words) > 50:
                return 1.8
            else:
                return 1.2
    
    def _build_evaluation_prompt(self, criterion_config: CriteriaConfig, example: Example) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏"""
        system_prompt = criterion_config.get("system_prompt", "")
        instruction = example.get('instruction', '')
        generated = example.get('generated', '')
        reference = example.get('reference', '')
        input_text = example.get('input', '')
        
        user_prompt = f"""
–ò–ù–°–¢–†–£–ö–¶–ò–Ø: {instruction}

{"–í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï: " + input_text if input_text else ""}

–û–¢–í–ï–¢ –î–õ–Ø –û–¶–ï–ù–ö–ò: {generated}

{"–≠–¢–ê–õ–û–ù–ù–´–ô –û–¢–í–ï–¢: " + reference if reference else ""}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å—Ç—Ä–æ–≥–æ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—é –∏ –≤–µ—Ä–¥–∏–∫—Ç –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å –æ—Ü–µ–Ω–∫—É –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:
"""
        
        return system_prompt + user_prompt
    
    def _generate_request_cache_key(self, criterion_config: CriteriaConfig, example: Example) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –¥–ª—è –∫—ç—à–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        key_data = {
            'system_prompt': criterion_config.get('system_prompt', ''),
            'instruction': example.get('instruction', ''),
            'generated': example.get('generated', ''),
            'reference': example.get('reference', '')
        }
        key_string = json.dumps(key_data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _get_default_model(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return None  # –ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    
    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à –æ—Ü–µ–Ω—â–∏–∫–∞"""
        self._request_cache.clear()

class WeightManager:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç –≤–µ—Å–∞–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self):
        self._cache = ThreadSafeCache(config.get('cache_size', 1000))
    
    def calculate_weights(self, criteria: List[str], task_type: str, generated_text: str) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
        cache_key = self._generate_cache_key(criteria, task_type, generated_text)
        
        cached_result = self._cache.get(cache_key)
        if cached_result is not None:
            return cached_result
        
        base_weights = self._get_base_weights(task_type, criteria)
        adjusted_weights = self._adjust_by_complexity(base_weights, generated_text)
        normalized_weights = self._normalize_weights(adjusted_weights)
        
        self._cache.set(cache_key, normalized_weights)
        return normalized_weights
    
    def _get_base_weights(self, task_type: str, criteria: List[str]) -> Dict[str, float]:
        """–ë–∞–∑–æ–≤—ã–µ –≤–µ—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏"""
        
        task_weights_config = {
            "static_benchmark": {
                "–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å": 0.7,
                "—Ñ–æ—Ä–º–∞—Ç": 0.3,
                "–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å": 0.0,
                "—Ä–µ—á–µ–≤—ã–µ_–æ—à–∏–±–∫–∏": 0.0,
                "–æ–±—â–µ–µ_–∫–∞—á–µ—Å—Ç–≤–æ": 0.0
            },
            "function_calling": {
                "–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å": 0.4,
                "—Ñ–æ—Ä–º–∞—Ç": 0.1,
                "–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å": 0.5,
                "—Ä–µ—á–µ–≤—ã–µ_–æ—à–∏–±–∫–∏": 0.0,
                "–æ–±—â–µ–µ_–∫–∞—á–µ—Å—Ç–≤–æ": 0.0
            },
            "generative": {
                "–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å": 0.3,
                "—Ñ–æ—Ä–º–∞—Ç": 0.1,
                "–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å": 0.1,
                "—Ä–µ—á–µ–≤—ã–µ_–æ—à–∏–±–∫–∏": 0.3,
                "–æ–±—â–µ–µ_–∫–∞—á–µ—Å—Ç–≤–æ": 0.2
            }
        }
        
        weights = task_weights_config.get(task_type, task_weights_config["generative"])
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏
        return {k: v for k, v in weights.items() if k in criteria}
    
    def _adjust_by_complexity(self, weights: Dict[str, float], text: str) -> Dict[str, float]:
        """–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        complexity = self._estimate_complexity(text)
        
        adjusted_weights = {}
        for criterion, weight in weights.items():
            if criterion == "—Ä–µ—á–µ–≤—ã–µ_–æ—à–∏–±–∫–∏" and complexity > 0.7:
                adjusted_weights[criterion] = weight * 1.3
            elif criterion == "–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å" and complexity > 0.5:
                adjusted_weights[criterion] = weight * 1.2
            elif criterion == "–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å" and complexity > 0.6:
                adjusted_weights[criterion] = weight * 1.1
            else:
                adjusted_weights[criterion] = weight
        
        return adjusted_weights
    
    @lru_cache(maxsize=1000)
    def _estimate_complexity(self, text: str) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
        if not text or len(text) > MAX_TEXT_LENGTH:
            return 0.0
        
        words = text.split()
        if len(words) < 10:
            return 0.3
        
        try:
            # –≠–≤—Ä–∏—Å—Ç–∏–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
            long_words = sum(1 for word in words if len(word) > 6)
            sentences = len(re.findall(r'[.!?]+', text))
            unique_words = len(set(words))
            
            # –ö–æ–º–ø–æ–∑–∏—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            lexical_complexity = long_words / len(words)
            syntactic_complexity = len(words) / max(sentences, 1) / 15  # —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
            vocabulary_richness = unique_words / len(words)
            
            complexity = min(1.0, 
                lexical_complexity * 0.4 + 
                syntactic_complexity * 0.4 + 
                vocabulary_richness * 0.2
            )
            return complexity
        except Exception:
            return 0.5
    
    def _normalize_weights(self, weights: Dict[str, float]) -> Dict[str, float]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤–µ—Å–∞ –∫ —Å—É–º–º–µ 1.0"""
        total = sum(weights.values())
        if total == 0:
            return {k: 1.0/len(weights) for k in weights.keys()}
        
        return {k: v/total for k, v in weights.items()}
    
    def _generate_cache_key(self, criteria: List[str], task_type: str, text: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –¥–ª—è –∫—ç—à–∞"""
        sorted_criteria = sorted(criteria)
        text_hash = hashlib.md5(text.encode()).hexdigest()[:8] if text else "empty"
        key_string = f"{'-'.join(sorted_criteria)}_{task_type}_{text_hash}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤–µ—Å–æ–≤"""
        self._cache.clear()
        self._estimate_complexity.cache_clear()

class VerdictValidator:
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –≤–µ—Ä–¥–∏–∫—Ç—ã –æ—Ü–µ–Ω–∫–∏"""
    
    def synthesize_verdicts(self, verdicts: Dict[str, EvaluationVerdict], example: Example) -> float:
        """–°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É –∏–∑ –≤–µ—Ä–¥–∏–∫—Ç–æ–≤"""
        if not verdicts:
            return 0.0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –≤—ã–±—Ä–æ—Å—ã
        outliers = self._detect_outliers(verdicts)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–∏–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã - –∑–∞–ø—É—Å–∫–∞–µ–º –∞—Ä–±–∏—Ç—Ä–∞
        if outliers and len(verdicts) > 2:
            return self._call_arbiter(verdicts, example, outliers)
        
        # –ò–Ω–∞—á–µ - –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ
        total_score = 0.0
        total_weight = 0.0
        
        for criterion_name, verdict in verdicts.items():
            if criterion_name in outliers:
                # –®—Ç—Ä–∞—Ñ—É–µ–º –≤—ã–±—Ä–æ—Å—ã
                weight = verdict['weight'] * 0.3
            else:
                weight = verdict['weight']
            
            total_score += verdict['score'] * weight
            total_weight += weight
        
        if total_weight == 0:
            return 0.0
        
        final_score = total_score / total_weight
        return min(2.0, max(0.0, final_score))
    
    def _detect_outliers(self, verdicts: Dict[str, EvaluationVerdict]) -> List[str]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–±—Ä–æ—Å—ã —Å—Ä–µ–¥–∏ –≤–µ—Ä–¥–∏–∫—Ç–æ–≤"""
        scores = [v['score'] for v in verdicts.values()]
        
        if len(scores) < 3:
            return []
        
        try:
            Q1 = np.percentile(scores, 25)
            Q3 = np.percentile(scores, 75)
            IQR = Q3 - Q1
            
            if IQR == 0:  # –í—Å–µ –æ—Ü–µ–Ω–∫–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ
                return []
            
            outliers = []
            for criterion_name, verdict in verdicts.items():
                score = verdict['score']
                if score < (Q1 - 1.5 * IQR) or score > (Q3 + 1.5 * IQR):
                    outliers.append(criterion_name)
            
            return outliers
        except Exception:
            return []
    
    def _call_arbiter(self, verdicts: Dict[str, EvaluationVerdict], example: Example, outliers: List[str]) -> float:
        """–í—ã–∑—ã–≤–∞–µ—Ç –∞—Ä–±–∏—Ç—Ä–∞ –¥–ª—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã—Ö –≤–µ—Ä–¥–∏–∫—Ç–æ–≤"""
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –º–µ–¥–∏–∞–Ω–∞ —Å —É—á–µ—Ç–æ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        scores_with_weights = []
        
        for criterion_name, verdict in verdicts.items():
            if criterion_name not in outliers:
                scores_with_weights.extend([verdict['score']] * int(verdict['weight'] * 10))
        
        if not scores_with_weights:
            scores_with_weights = [v['score'] for v in verdicts.values()]
        
        try:
            return float(np.median(scores_with_weights))
        except Exception:
            return sum(scores_with_weights) / len(scores_with_weights)

class FoundationEvaluationSystem:
    """
    –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –ò–ò —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ - –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∞—è –≤–µ—Ä—Å–∏—è
    """
    
    def __init__(self, base_model=None, fallback_strategy: ModelFallbackStrategy = ModelFallbackStrategy.RETRY):
        self.synthesizer = CriteriaSynthesizer()
        self.evaluator = CriteriaEvaluator(base_model, fallback_strategy)
        self.validator = VerdictValidator()
        self.weight_manager = WeightManager()
        self.batch_processor = BatchProcessor(config.get('batch_size', 10))
        self.academic_metrics = AcademicEvaluationMetrics()
        
        self.metrics = ProcessingMetrics()
        self.performance_stats = PerformanceStats()
        self.comprehensive_metrics = ComprehensiveMetrics()
        self.logger = logging.getLogger(__name__)
        
        self._executor = ThreadPoolExecutor(
            max_workers=config.get('max_workers', 4),
            thread_name_prefix="eval_worker"
        )
        self._metrics_lock = Lock()
        self._detailed_verdicts = []  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    async def predict_async(self, examples: List[Dict]) -> List[float]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫"""
        self._validate_input_examples(examples)
        
        with self._metrics_lock:
            self.metrics.total_examples = len(examples)
            self._detailed_verdicts = []  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        
        start_time = time.time()
        
        try:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏ —á–µ—Ä–µ–∑ BatchProcessor
            results = await self.batch_processor.process_batches_async(
                examples, 
                self._process_batch_async
            )
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            with self._metrics_lock:
                self.performance_stats.total_processing_time = time.time() - start_time
                self.performance_stats.examples_processed = len(examples)
            
            # –†–∞—Å—á–µ—Ç –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫ –µ—Å–ª–∏ –µ—Å—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏
            if config.get('academic_metrics', True):
                human_scores = [ex.get('human_score') for ex in examples if ex.get('human_score') is not None]
                if len(human_scores) == len(results):
                    self.comprehensive_metrics = self.academic_metrics.compute_comprehensive_metrics(
                        results, human_scores, self._detailed_verdicts
                    )
            
            self.logger.info(f"–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {self.metrics.successful_processing}/{self.metrics.total_examples} –ø—Ä–∏–º–µ—Ä–æ–≤")
            self.logger.info(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {self.performance_stats.avg_processing_time:.3f}—Å")
            
            return results
            
        except Exception as e:
            self.logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
            raise EvaluationError(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏–º–µ—Ä–æ–≤: {e}")
    
    async def _process_batch_async(self, examples: List[Example]) -> List[float]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –ø—Ä–∏–º–µ—Ä–æ–≤"""
        tasks = [self._process_single_example_async(example) for example in examples]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞: {result}")
                processed_results.append(ERROR_FALLBACK_SCORE)
                with self._metrics_lock:
                    self.metrics.error_count += 1
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _process_single_example_async(self, example: Example) -> float:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞"""
        try:
            start_time = time.time()
            
            # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            criteria_start = time.time()
            criteria = self._extract_criteria(example)
            task_type = self._detect_task_type(example)
            with self._metrics_lock:
                self.performance_stats.criteria_extraction_time += time.time() - criteria_start
            
            # 2. –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏
            criteria_config = self.synthesizer.create_criteria(criteria, task_type)
            
            # 3. –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            evaluation_start = time.time()
            raw_verdicts = await self.evaluator.evaluate_criteria_async(criteria_config, example)
            with self._metrics_lock:
                self.performance_stats.criteria_evaluation_time += time.time() - evaluation_start
            
            # 4. –†–∞—Å—á–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –≤–µ—Å–æ–≤
            weight_calc_start = time.time()
            weights = self.weight_manager.calculate_weights(
                criteria, task_type, example.get('generated', '')
            )
            with self._metrics_lock:
                self.performance_stats.weight_calculation_time += time.time() - weight_calc_start
            
            # 5. –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            weighted_verdicts = self._apply_weights_and_confidence(raw_verdicts, weights)
            
            # 6. –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Å–∏–Ω—Ç–µ–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
            validation_start = time.time()
            final_raw_score = self.validator.synthesize_verdicts(weighted_verdicts, example)
            with self._metrics_lock:
                self.performance_stats.validation_time += time.time() - validation_start
            
            # 7. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –º–µ—Ç—Ä–∏–∫—É –ø–æ —Ñ–æ—Ä–º—É–ª–µ –∫–æ–Ω–∫—É—Ä—Å–∞
            metric = self._convert_to_metric(final_raw_score, example, task_type)
            
            # 8. –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫
            detailed_verdict = {
                'final_score': metric,
                'confidence': self._calculate_confidence_aggregate(weighted_verdicts),
                'reasoning': ' | '.join([v.get('reasoning', '') for v in weighted_verdicts.values()]),
                **weighted_verdicts
            }
            
            with self._metrics_lock:
                self.metrics.successful_processing += 1
                self._detailed_verdicts.append(detailed_verdict)
            
            return round(metric, 4)
            
        except (ValueError, TypeError, KeyError) as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏–º–µ—Ä–∞: {e}")
            with self._metrics_lock:
                self.metrics.error_count += 1
            return ERROR_FALLBACK_SCORE
        except Exception as e:
            self.logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏–º–µ—Ä–∞: {e}")
            with self._metrics_lock:
                self.metrics.error_count += 1
            return ERROR_FALLBACK_SCORE
    
    def _calculate_confidence_aggregate(self, verdicts: Dict[str, EvaluationVerdict]) -> float:
        """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∏–∑ –≤—Å–µ—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
        confidences = [v.get('confidence', DEFAULT_CONFIDENCE) for v in verdicts.values()]
        return np.mean(confidences) if confidences else DEFAULT_CONFIDENCE
    
    def _validate_input_examples(self, examples: List[Dict]):
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã"""
        if not examples:
            raise ValidationError("–°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        
        required_fields = ['instruction', 'generated', 'criterion']
        for i, example in enumerate(examples):
            for field in required_fields:
                if field not in example:
                    raise ValidationError(f"–ü—Ä–∏–º–µ—Ä {i} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ: {field}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
            for text_field in ['instruction', 'generated']:
                text = example.get(text_field, '')
                if len(text) > MAX_TEXT_LENGTH:
                    raise ValidationError(f"–ü–æ–ª–µ {text_field} –≤ –ø—Ä–∏–º–µ—Ä–µ {i} –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É")
    
    def predict(self, examples: List[Dict]) -> List[float]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        try:
            return asyncio.run(self.predict_async(examples))
        except RuntimeError:
            # –ï—Å–ª–∏ —É–∂–µ –∑–∞–ø—É—â–µ–Ω event loop
            loop = asyncio.get_event_loop()
            return loop.run_until_complete(self.predict_async(examples))
    
    def get_detailed_verdicts(self) -> List[Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä–¥–∏–∫—Ç—ã –¥–ª—è –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        with self._metrics_lock:
            return self._detailed_verdicts.copy()
    
    def get_academic_report(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç"""
        performance_report = self.get_performance_report()
        
        return {
            'performance_metrics': performance_report,
            'academic_metrics': self.comprehensive_metrics.to_dict(),
            'composite_scores': {
                'overall_quality_score': self.comprehensive_metrics.overall_quality,
                'evaluation_system_quality': self.comprehensive_metrics.evaluation_system_quality,
                'academic_composite_score': self._calculate_academic_composite_score()
            }
        }
    
    def _calculate_academic_composite_score(self) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π score"""
        weights = {
            'spearman_correlation': 0.25,
            'overall_quality': 0.2,
            'mean_absolute_error': 0.15,
            'confidence_calibration': 0.1,
            'system_reliability': 0.1,
            'exact_match_rate': 0.1,
            'evaluation_consistency': 0.1
        }
        
        metrics = self.comprehensive_metrics
        composite = 0.0
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –≤–∑–≤–µ—à–∏–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        composite += weights['spearman_correlation'] * metrics.spearman_correlation
        composite += weights['overall_quality'] * metrics.overall_quality
        composite += weights['mean_absolute_error'] * max(0, 1 - metrics.mean_absolute_error)
        composite += weights['confidence_calibration'] * metrics.confidence_calibration
        composite += weights['system_reliability'] * metrics.system_reliability
        composite += weights['exact_match_rate'] * metrics.exact_match_rate
        composite += weights['evaluation_consistency'] * metrics.evaluation_consistency
        
        return composite

    def _extract_criteria(self, example: Example) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ –∏–∑ –ø—Ä–∏–º–µ—Ä–∞"""
        criterion = example.get('criterion', '').lower()
        scale = example.get('scale', '').lower()
        
        criteria = []
        if "–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å" in criterion and "—Ñ–æ—Ä–º–∞—Ç" in criterion:
            criteria.extend(["–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å", "—Ñ–æ—Ä–º–∞—Ç"])
        elif "–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å" in criterion:
            criteria.append("–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å")
        elif "—Ñ–æ—Ä–º–∞—Ç" in criterion:
            criteria.append("—Ñ–æ—Ä–º–∞—Ç")
        elif "–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å" in criterion:
            criteria.append("–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å")
        elif "—Ä–µ—á–µ–≤—ã–µ –æ—à–∏–±–∫–∏" in criterion or "—Ä–µ—á–µ–≤—ã–µ" in criterion:
            criteria.append("—Ä–µ—á–µ–≤—ã–µ_–æ—à–∏–±–∫–∏")
        elif "–∫–∞—á–µ—Å—Ç–≤–æ" in criterion or "–æ–±—â–µ–µ" in criterion:
            criteria.append("–æ–±—â–µ–µ_–∫–∞—á–µ—Å—Ç–≤–æ")
        else:
            # –ê–Ω–∞–ª–∏–∑ scale –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
            if "—Ñ–æ—Ä–º–∞—Ç" in scale:
                criteria.append("—Ñ–æ—Ä–º–∞—Ç")
            if "–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å" in scale or "—Ç–æ—á–Ω–æ—Å—Ç—å" in scale:
                criteria.append("–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å")
            if not criteria:
                criteria.append("–æ–±—â–µ–µ_–∫–∞—á–µ—Å—Ç–≤–æ")
        
        return criteria
    
    def _detect_task_type(self, example: Example) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–∞–¥–∞—á–∏"""
        instruction = example.get('instruction', '').lower()
        generated = example.get('generated', '').lower()
        input_text = example.get('input', '').lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ (–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä)
        if any(word in instruction for word in ['–≤—ã–±–µ—Ä–∏', '–≤–∞—Ä–∏–∞–Ω—Ç', 'a)', 'b)', 'c)', 'd)', '–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç']):
            return TaskType.STATIC_BENCHMARK.value
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π
        elif any(word in instruction for word in ['—Ñ—É–Ω–∫—Ü–∏—è', '—Ñ—É–Ω–∫—Ü–∏—é', '–≤—ã–∑–≤–∞—Ç—å', '–≤—ã–∑–æ–≤']):
            return TaskType.FUNCTION_CALLING.value
        elif any(word in generated for word in ['—Ñ—É–Ω–∫—Ü–∏—è', 'def ', 'function', '()']):
            return TaskType.FUNCTION_CALLING.value
        # –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏
        else:
            return TaskType.GENERATIVE.value
    
    def _apply_weights_and_confidence(self, verdicts: Dict[str, EvaluationVerdict], weights: Dict[str, float]) -> Dict[str, EvaluationVerdict]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤–µ—Å–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–æ–∫"""
        adjusted_verdicts = {}
        
        for criterion_name, verdict in verdicts.items():
            base_weight = weights.get(criterion_name, 0.5)
            confidence = verdict.get('confidence', DEFAULT_CONFIDENCE)
            
            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            adjusted_weight = base_weight * (0.5 + 0.5 * confidence)
            verdict['weight'] = adjusted_weight
            adjusted_verdicts[criterion_name] = verdict
        
        return adjusted_verdicts
    
    def _convert_to_metric(self, raw_score: float, example: Example, task_type: str) -> float:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å—ã—Ä—É—é –æ—Ü–µ–Ω–∫—É –≤ –º–µ—Ç—Ä–∏–∫—É –ø–æ —Ñ–æ—Ä–º—É–ª–µ M = 2 - (raw_score / max_score)"""
        if raw_score == -1:
            return ERROR_FALLBACK_SCORE
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª –¥–ª—è —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
        if task_type == TaskType.STATIC_BENCHMARK.value:
            max_score = 3
        elif task_type == TaskType.FUNCTION_CALLING.value:
            max_score = 2
        else:  # generative
            max_score = 2
        
        metric = 2 - (raw_score / max_score)
        return max(0.0, min(metric, 2.0))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω
    
    def get_performance_report(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        with self._metrics_lock:
            success_rate = self.metrics.successful_processing / max(self.metrics.total_examples, 1)
            
            return {
                'processing_metrics': self.performance_stats.to_dict(),
                'success_rate': success_rate,
                'error_count': self.metrics.error_count,
                'total_examples': self.metrics.total_examples,
                'successful_processing': self.metrics.successful_processing,
                'model_timeouts': self.metrics.model_timeouts
            }
    
    def clear_state(self):
        """–û—á–∏—â–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã"""
        with self._metrics_lock:
            self.metrics = ProcessingMetrics()
            self.performance_stats = PerformanceStats()
            self._detailed_verdicts = []
        
        self.weight_manager.clear_cache()
        self.synthesizer.clear_cache()
        self.evaluator.clear_cache()
    
    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if hasattr(self, '_executor'):
            self._executor.shutdown(wait=False)

# –£—Ç–∏–ª–∏—Ç—ã
def setup_logging(level=logging.INFO):
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã"""
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('foundation_ai_evaluation.log', encoding='utf-8')
        ]
    )

def create_config_file():
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
    config_example = {
        'model_timeout': 30,
        'max_retries': 3,
        'batch_size': 10,
        'performance_metrics': True,
        'fallback_strategy': 'retry',
        'max_workers': 4,
        'cache_size': 1000,
        'academic_metrics': True,
        
        'task_weights': {
            'static_benchmark': {
                '–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å': 0.7,
                '—Ñ–æ—Ä–º–∞—Ç': 0.3
            },
            'function_calling': {
                '–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å': 0.4,
                '—Ñ–æ—Ä–º–∞—Ç': 0.1,
                '–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å': 0.5
            },
            'generative': {
                '–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å': 0.3,
                '—Ñ–æ—Ä–º–∞—Ç': 0.1,
                '–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å': 0.1,
                '—Ä–µ—á–µ–≤—ã–µ_–æ—à–∏–±–∫–∏': 0.3,
                '–æ–±—â–µ–µ_–∫–∞—á–µ—Å—Ç–≤–æ': 0.2
            }
        }
    }
    
    with open(CONFIG_PATH, 'w', encoding='utf-8') as f:
        yaml.dump(config_example, f, allow_unicode=True, default_flow_style=False)

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
async def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã —Å –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    setup_logging()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if not CONFIG_PATH.exists():
        create_config_file()
        print("–°–æ–∑–¥–∞–Ω –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª config.yaml")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —Å fallback —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
    evaluation_system = FoundationEvaluationSystem(fallback_strategy=ModelFallbackStrategy.MOCK)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    test_examples = [
        {
            "instruction": "–ù–∞–π–¥–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è: 25 + 37. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º.",
            "generated": "62", 
            "reference": "62",
            "criterion": "–ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∞",
            "scale": "0: –æ—à–∏–±–∫–∏, 1: –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –Ω–æ –æ—à–∏–±–∫–∞, 2: –ø—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–æ –Ω–µ —Ñ–æ—Ä–º–∞—Ç, 3: –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–∞–≤–∏–ª—å–Ω–æ",
            "human_score": 3.0
        },
        {
            "instruction": "–ù–∞–ø–∏—à–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é —Å–ª–æ–∂–µ–Ω–∏—è –¥–≤—É—Ö —á–∏—Å–µ–ª –Ω–∞ Python",
            "generated": "def add(a, b): return a + b",
            "reference": "def add(a, b):\n    return a + b",
            "criterion": "–ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å",
            "scale": "0-2",
            "human_score": 1.8
        },
        {
            "instruction": "–û–ø–∏—à–∏—Ç–µ –ø—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã —Å–æ–ª–Ω–µ—á–Ω–æ–π –±–∞—Ç–∞—Ä–µ–∏",
            "generated": "–°–æ–ª–Ω–µ—á–Ω–∞—è –±–∞—Ç–∞—Ä–µ—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–æ–ª–Ω–µ—á–Ω—É—é —ç–Ω–µ—Ä–≥–∏—é –≤ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫—É—é —Å –ø–æ–º–æ—â—å—é —Ñ–æ—Ç–æ—ç–ª–µ–º–µ–Ω—Ç–æ–≤",
            "reference": "–°–æ–ª–Ω–µ—á–Ω—ã–µ –ø–∞–Ω–µ–ª–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç —Å–æ–ª–Ω–µ—á–Ω—ã–π —Å–≤–µ—Ç –≤ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ —á–µ—Ä–µ–∑ —Ñ–æ—Ç–æ—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç",
            "criterion": "–ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–ª–æ–∂–µ–Ω–∏—è",
            "scale": "0-2",
            "human_score": 1.5
        }
    ] * 3  # –£–º–Ω–æ–∂–∞–µ–º –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
    
    print("–ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏...")
    
    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    start_time = time.time()
    results = await evaluation_system.predict_async(test_examples)
    processing_time = time.time() - start_time
    
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏: {results}")
    print(f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.3f}—Å")
    
    # –ü–æ–ª–Ω—ã–π –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç
    academic_report = evaluation_system.get_academic_report()
    print("\n=== –ü–û–õ–ù–´–ô –ê–ö–ê–î–ï–ú–ò–ß–ï–°–ö–ò–ô –û–¢–ß–ï–¢ ===")
    print(json.dumps(academic_report, indent=2, ensure_ascii=False, default=str))
    
    # –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    comp_metrics = evaluation_system.comprehensive_metrics
    print(f"\n=== –ö–õ–Æ–ß–ï–í–´–ï –ú–ï–¢–†–ò–ö–ò ===")
    print(f"–ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π score: {evaluation_system._calculate_academic_composite_score():.3f}")
    print(f"–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å —á–µ–ª–æ–≤–µ–∫–æ–º (Spearman): {comp_metrics.spearman_correlation:.3f}")
    print(f"–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {comp_metrics.mean_absolute_error:.3f}")
    print(f"–ö–∞—á–µ—Å—Ç–≤–æ —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏: {comp_metrics.evaluation_system_quality:.3f}")
    
    # –û—á–∏—Å—Ç–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    evaluation_system.clear_state()

if __name__ == "__main__":
    asyncio.run(main())
